
@incollection{NIPS2012_4687,
title = {Large Scale Distributed Deep Networks},
author = {Jeffrey Dean and Greg Corrado and Rajat Monga and Chen, Kai and Matthieu Devin and Mark Mao and Marc
aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Quoc V. Le and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1223--1231},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf}
}

@inproceedings{lane2015early,
  title={An early resource characterization of deep learning on wearables, smartphones and internet-of-things devices},
  author={Lane, Nicholas D and Bhattacharya, Sourav and Georgiev, Petko and Forlivesi, Claudio and Kawsar, Fahim},
  booktitle={Proceedings of the 2015 International Workshop on Internet of Things towards Applications},
  pages={7--12},
  year={2015},
  organization={ACM}
}

@article{Han2015,
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J.},
doi = {abs/1510.00149/1510.00149},
eprint = {1510.00149},
file = {:Users/dong/Dropbox/EIT/2017-18{\_}WS{\_}AALTO/Seminar in Computer Science/Learning phase2/1510.00149.pdf:pdf},
mendeley-groups = {Training on mobile},
pages = {1--14},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
url = {http://arxiv.org/abs/1510.00149},
year = {2015}
}

@article{Suresh2016,
abstract = {Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike previous works, we make no probabilistic assumptions on the data. We first show that for {\$}d{\$} dimensional data with {\$}n{\$} clients, a naive stochastic binary rounding approach yields a mean squared error (MSE) of {\$}\backslashTheta(d/n){\$} and uses a constant number of bits per dimension per client. We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to {\$}\backslashmathcal{\{}O{\}}((\backslashlog d)/n){\$} and a better coding strategy further reduces the error to {\$}\backslashmathcal{\{}O{\}}(1/n){\$} and uses a constant number of bits per dimension per client. We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd's algorithm for k-means and power iteration for PCA.},
archivePrefix = {arXiv},
arxivId = {1611.00429},
author = {Suresh, Ananda Theertha and Yu, Felix X. and Kumar, Sanjiv and McMahan, H. Brendan},
eprint = {1611.00429},
file = {:Users/dong/Dropbox/EIT/2017-18{\_}WS{\_}AALTO/Seminar in Computer Science/Learning phase2/1611.00429.pdf:pdf},
mendeley-groups = {Training on mobile},
pages = {1--17},
title = {{Distributed Mean Estimation with Limited Communication}},
url = {http://arxiv.org/abs/1611.00429},
year = {2016}
}


@misc{AppleInc.,
author = {{Apple Inc.}},
mendeley-groups = {Training on mobile},
title = {{Core ML}},
howpublished = {http://www.gartner.com/newsroom/id/3415117},
note = {2017-10-01}
}


@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{MobileNets2017,
  author    = {Andrew G. Howard and
               Menglong Zhu and
               Bo Chen and
               Dmitry Kalenichenko and
               Weijun Wang and
               Tobias Weyand and
               Marco Andreetto and
               Hartwig Adam},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
               Applications},
  journal   = {CoRR},
  volume    = {abs/1704.04861},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04861},
  archivePrefix = {arXiv},
  eprint    = {1704.04861},
  timestamp = {Wed, 07 Jun 2017 14:40:11 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/HowardZCKWWAA17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, Christopher JC},
  journal={AT\&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},
  volume={2},
  year={2010}
}

@misc{BrendanMcMahanandDanielRamage2017,
author = {{Brendan McMahan and Daniel Ramage}},
booktitle = {Google Research Blog},
mendeley-groups = {Training on mobile},
title = {{Research Blog: Federated Learning: Collaborative Machine Learning without Centralized Training Data}},
url = {https://research.googleblog.com/2017/04/federated-learning-collaborative.html},
urldate = {2017-11-12},
year = {2017}
}



@article{Shamir2013,
abstract = {We present a novel Newton-type method for distributed optimization, which is particularly well suited for stochastic optimization and learning problems. For quadratic objectives, the method enjoys a linear rate of convergence which provably $\backslash$emph{\{}improves{\}} with the data size, requiring an essentially constant number of iterations under reasonable assumptions. We provide theoretical and empirical evidence of the advantages of our method compared to other approaches, such as one-shot parameter averaging and ADMM.},
archivePrefix = {arXiv},
arxivId = {1312.7853},
author = {Shamir, Ohad and Srebro, Nathan and Zhang, Tong},
eprint = {1312.7853},
file = {:Users/dong/Dropbox/EIT/2017-18{\_}WS{\_}AALTO/Seminar in Computer Science/Learning phase2/shamir14.pdf:pdf},
isbn = {9781634393973},
journal = {Icml},
keywords = {method,munication-efficient distributed optimization,using an approximate newton-type},
mendeley-groups = {Training on mobile},
number = {4},
pages = {1--19},
title = {{Communication Efficient Distributed Optimization using an Approximate Newton-type Method}},
url = {http://arxiv.org/abs/1312.7853},
volume = {32},
year = {2013}
}



@article{Zinkevich2010,
abstract = {With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].},
author = {Zinkevich, Martin and Alex, Snikam and Weimer, Markus and Li, Lihong},
doi = {10.1088/0741-3335/38/11/011},
file = {:Users/dong/Dropbox/EIT/2017-18{\_}WS{\_}AALTO/Seminar in Computer Science/Learning phase2/nips2010.pdf:pdf},
issn = {0741-3335},
journal = {Advances in Neural Information Processing Systems 23},
mendeley-groups = {Training on mobile},
pages = {2595--2603},
pmid = {12484348},
title = {{Parallelized stochastic gradient descent}},
url = {http://martin.zinkevich.org/publications/nips2010.pdf},
year = {2010}
}

@article{Distillation:2015,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Xi Wu and
               Somesh Jha and
               Ananthram Swami},
  title     = {Distillation as a Defense to Adversarial Perturbations against Deep
               Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1511.04508},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.04508},
  archivePrefix = {arXiv},
  eprint    = {1511.04508},
  timestamp = {Wed, 07 Jun 2017 14:43:10 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/PapernotMWJS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}


@article{Ota:2017,
 author = {Ota, Kaoru and Dao, Minh Son and Mezaris, Vasileios and Natale, Francesco G. B. De},
 title = {Deep Learning for Mobile Multimedia: A Survey},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 issue_date = {August 2017},
 volume = {13},
 number = {3s},
 month = jun,
 year = {2017},
 issn = {1551-6857},
 pages = {34:1--34:22},
 articleno = {34},
 numpages = {22},
 url = {http://doi.acm.org/10.1145/3092831},
 doi = {10.1145/3092831},
 acmid = {3092831},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Deep learning, deep neural networks, mobile multimedia computing},
} 



@InProceedings{Warren1943,
  author =	 {Warren S. McCulloch and Walter Pitts},
  title =	 {A logical calculus of the ideas immanent in nervous activity},
  booktitle =	 { Bull. Math. Biophys},
  pages =	 {115-133},
  year =	 {1943},
  volume =	 {5}
}

@InProceedings{Jurgen2015,
  author =	 {Jürgen Schmidhuber},
  title =	 {Deep learning in neural networks: An overview},
  booktitle =	 {Neural Networks},
  pages =	 {85-117},
  year =	 {2015},
  volume =	 {61},
  month =	 {January}
}


@article{MAL-006,
url = {http://dx.doi.org/10.1561/2200000006},
year = {2009},
volume = {2},
journal = {Foundations and Trends® in Machine Learning},
title = {Learning Deep Architectures for AI},
doi = {10.1561/2200000006},
issn = {1935-8237},
number = {1},
pages = {1-127},
author = {Yoshua Bengio}
}

@INPROCEEDINGS{GoogLeNet, 
author={C. Szegedy and Wei Liu and Yangqing Jia and P. Sermanet and S. Reed and D. Anguelov and D. Erhan and V. Vanhoucke and A. Rabinovich}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Going deeper with convolutions}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-9}, 
keywords={Hebbian learning;convolution;decision making;feature extraction;image classification;neural net architecture;resource allocation;Hebbian principle;architectural decision;convolutional neural network architecture;object classification;object detection;resource utilization;Computer architecture;Computer vision;Convolutional codes;Neural networks;Object detection;Sparse matrices;Visualization}, 
doi={10.1109/CVPR.2015.7298594}, 
ISSN={1063-6919}, 
month={June},}

@INPROCEEDINGS{ResNet, 
author={K. He and X. Zhang and S. Ren and J. Sun}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Deep Residual Learning for Image Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={770-778}, 
keywords={image classification;learning (artificial intelligence);neural nets;object detection;CIFAR-10;COCO object detection dataset;COCO segmentation;ILSVRC & COCO 2015 competitions;ILSVRC 2015 classification task;ImageNet dataset;ImageNet localization;ImageNet test set;VGG nets;deep residual learning;deep residual nets;deeper neural network training;image recognition;residual function learning;residual nets;visual recognition tasks;Complexity theory;Degradation;Image recognition;Image segmentation;Neural networks;Training;Visualization}, 
doi={10.1109/CVPR.2016.90}, 
ISSN={}, 
month={June},}

@misc{moblePhoneSale,
  author = {Egham},
  title = {Gartner Says Five of Top 10 Worldwide Mobile Phone Vendors Increased Sales in Second Quarter of 2016},
  year = 2016,
  howpublished = {http://www.gartner.com/newsroom/id/3415117},
  note = {2017-10-01}
}


% Seminar on Network Security, BiBTeX template 
% Example entries 

@Book{Com00,
  author =	 {Douglas E. Comer},
  title =	 {Internetworking with TCP/IP, Volume I},
  publisher =	 {Prentice-Hall Inc},
  year =	 {2000},
  edition =	 {4th},
  annote =	 {ISBN: 0-13-018380-6},
  annote =	 {Good basic book about TCP/IP and its applications;
                  Includes IP, ARP, ICMP, CIDR, TCP, UDP, Routing with
                  BGP, RIP, OSPF, HELLO, Multicast, ATM, Mobile IP,
                  NAT, VPN, Sockets, DHCP, DNS, Telnet, FTP,
                  Electronic Mail, WWW, VoIP, SNMP, IPsec, IPv6, but
                  only in the basic level for students}
}

@InProceedings{HTS03,
  author =	 {{Tim Hsin-ting Hu and Binh Thai and Aruna
                  Seneviratne}},
  title =	 {{Supporting Mobile Devices in Gnutella File 
                  Sharing Network with Mobile Agents}},
  booktitle =	 {IEEE International Symposium on Computers
                  and Communication},
  pages =	 {1530-1346},
  year =	 {2003},
  volume =	 {3},
  month =	 {April}
}

@PhdThesis{Nik99,
  author =	 {Pekka Nikander},
  title =	 {An Architecture for Authorization and Delegation
                  in Distributed Object-Oriented Agent Systems},
  school =	 {Helsinki University of Technology},
  year =	 {1999},
  month =	 {March},
  annote =	 {Describes distributed software architecture that
                  uses delegated access permissions. Delegating
                  authorizations to Java agents}
}

@TechReport{RFC2408,
  author =	 {D. Maughan and M. Schertler and M. Schneider and
                  J. Turner},
  title =	 {{Internet Security Association and Key
                  Management Protocol (ISAKMP)}},
  institution =	 {The Internet Engineering Task Force},
  year =	 {1998},
  key =		 {rfc2408},
  type =	 {RFC},
  number =	 {2408},
  month =	 {November},
  note =	 {http://ietf.org/rfc/rfc2408.txt}
}

@MastersThesis{Suo98,
  author =	 {Sanna Suoranta},
  title =	 {{An Object-Oriented Implementation of an
                  Authentication Protocol}},
  school =	 {Helsinki University of Technology},
  year =	 {1998},
  month =	 {November},
  annote =	 {An ISAKMP implementation using Jacob framework
                  implementation that is based on conduits+
                  framework.}
}

@article{Caruana2006,
abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
author = {Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
doi = {10.1145/1150402.1150464},
file = {:Users/dong/Dropbox/EIT/2017-18{\_}WS{\_}AALTO/Seminar in Computer Science/Learning phase2/compression.kdd06.pdf:pdf},
isbn = {1595933395},
journal = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '06},
keywords = {model compression,supervised learning},
pages = {535},
title = {{Model compression}},
url = {http://portal.acm.org/citation.cfm?doid=1150402.1150464},
year = {2006}
}

@Book{DigitalDesign,
  author =	 {Harris Money David and Harris L. Sarah},
  title =	 {Digital design and computer architecture},
  publisher =	 {Morgan Kaufmann},
  year =	 {2012},
  edition =	 {2nd},
  annote =	 {ISBN: 978-0-12-394424-5},
}


@article{SoftMax,
author = {J. S. Bridle},
doi = {10.1145/1150402.1150464},
publisher =	 {Springer-Verlag},
annote =	 {Editors: Fogelman Soulie, Francoise and Herault, Jeanny},
journal = {Neurocomputing: Algorithms, Architectures and Applications},
keywords = {model compression,supervised learning},
pages = {227–236},
title = {{Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition}},
year = {1990},
annote =	 {ISBN: 978-3-642-76153-9}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The most common BiBTeX entry types
%
%    * Article: An article from a journal or magazine.
% 
%    * Book:  A book with an explicit publisher.
%
%    * Booklet:  A work that is printed and bound, but without 
%               a named publisher or sponsoring institution.
%
%    * Conference: An article in the proceedings of a conference. 
%                  This entry is identical to the 'inproceedings' 
%                  entry and is included for compatibility with another 
%                  text formatting system.
%
%    * Inbook: A part of a book, which may be a chapter and/or a 
%              range of pages.
%
%    * Incollection: A part of a book having its own title.
%
%    * Inproceedings: An article in a conference proceedings.
%
%    * Manual: Technical documentation.
%
%    * Mastersthesis: A Master's thesis.
%
%    * Misc: This is used when other entries do not fit.
%
%    * Phdthesis: A PhD thesis.
%
%    * Proceedings: The proceedings of a conference.
%
%    * Techreport: A report published by a school or other institution, 
%                  usually numbered within a series.
%
%    * Unpublished: A document having an author and title, but not 
%                   formally published.
%
%
% There are a huge number of BiBTeX guides online, more or less official.
