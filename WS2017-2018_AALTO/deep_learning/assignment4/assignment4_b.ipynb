{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with your name and student number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pagebreak$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Exercise on Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A confounding problem in Machine Learning is how to construct a model that will perform well not just on the training data, but also on data that was not seen by the model during training. If the model fits the training data but does not have a good predicting performance/generalisation power we have an overfitting problem. Regularisation is a technique used to avoid the overfitting problem. Models that overfit data are complex models that have too many or too large magnitude parameters. The idea behind regularisation is to discourage complex models by penalising large magnitude or too many parameters with higher cost.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>All methods that lead to improving the accuracy on the test set, possibly at the expense of increasing the training error, can be called as regularisation. To understand its effects and benefits, you will implement <b>L2 regularisation</b> in this exercise. This exercise is the second part of this week's exercises. The maximum points of this part is 2 points.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will begin by first importing the necessary python libraries.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import pp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "print('***** Import complete *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We are going to generate data for this exercise by ourselves. The dataset will consist only of points in a 2D-plane and the points belong to 3 different classes. For each class, there are 500 samples. In the following cell, we set the random seed for uniformity, make some intialisations for the data generation and generate the data. You can just run the cell to generate the data and do not really have to understand how it is generated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(234)\n",
    "\n",
    "# Intialization of some parameters\n",
    "\n",
    "# Number of input samples in each class\n",
    "NX = 500 \n",
    "# Dimension of the input samples, use 2 so that we can visualize\n",
    "DX = 2\n",
    "# Number of different classes\n",
    "CL = 3 \n",
    "\n",
    "# Template for the dataset\n",
    "\n",
    "# Data features\n",
    "X_data = np.zeros((NX * CL, DX))\n",
    "# Class labels\n",
    "y_data = np.zeros(NX * CL, dtype = 'uint8')\n",
    "\n",
    "# Generation of spiral data\n",
    "for j in range(CL):\n",
    "    ix = range(NX*j,NX*(j+1))\n",
    "    r = np.linspace(0.0,1,NX)\n",
    "    t = np.linspace(j*4,(j+1)*4,NX) + np.random.randn(NX)*0.2\n",
    "    X_data[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y_data[ix] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It is a good practice to shuffle the data before we split it in to training set and test set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "m = X_data.shape[0]\n",
    "# Shuffle the indices to the data\n",
    "permutation = list(np.random.permutation(m))\n",
    "X_data = X_data[permutation]\n",
    "y_data = y_data[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can visualise the data. You will see three different classes forming three spiral-like forms. This kind of data can not be separated with linear classifiers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the input data and classes\n",
    "hplot = 0.02\n",
    "x_min, x_max = X_data[:, 0].min() - 1, X_data[:, 0].max() + 1\n",
    "y_min, y_max = X_data[:, 1].min() - 1, X_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, hplot),\n",
    "                     np.arange(y_min, y_max, hplot))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], c=y_data, s=10, cmap=plt.cm.cool)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title('Original Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In order to make a more complex decision boundary for our assignment, we will add some noise into the data set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some Gaussian noise\n",
    "input_noise = np.random.normal(loc=0., scale=0.2, size=[NX*CL, DX])\n",
    "# Adding noise to the data\n",
    "X_data = X_data + input_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We intentionally use a small training set for this exercise so that we can practice how to avoid overfitting with regularisation. The effects of overfitting tend to be more pronounced when we have a small training dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training set\n",
    "# Let us make a small training set, simulating a situation\n",
    "# when we do not have a lot of training samples and thus a\n",
    "# danger to overfit\n",
    "\n",
    "# Training set dimensions: (200 x 2)\n",
    "X = X_data[0:200]\n",
    "\n",
    "# Training set labels dimensions: (200 x 1)\n",
    "y = y_data[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will keep the test set relatively large so that we get reliable results for the test error.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the test set\n",
    "# More data in the test set to get more reliable results\n",
    "\n",
    "# Test set dimensions: (1300 x 2)\n",
    "X_test = X_data[200:]\n",
    "\n",
    "# Test set labels dimensions: (1300 x 1)\n",
    "y_test = y_data[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can plot the training set that has only 200 training samples. You can still see a spiral-like shape, but the data is rather noisy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the training data and classes\n",
    "hplot = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, hplot),\n",
    "                     np.arange(y_min, y_max, hplot))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=plt.cm.cool)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Training Set with Gaussian noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And now we can also plot the noisy test data to see how it looks like.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the test data and classes\n",
    "hplot = 0.02\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, hplot),\n",
    "                     np.arange(y_min, y_max, hplot))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=10, cmap=plt.cm.cool)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title('Testing Set with Gaussian Noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The idea of regularisation is to augment our loss function by adding a complexity term that would give a bigger loss for more complex models. Let $\\theta_w$ represent the weights to be optimised excluding biases, $\\mathcal{L}(\\theta, \\mathcal{D})$ be the cost function and $\\lambda$ be the regularisation coefficient. We can write the L2 regularised cost function as follows:</p>\n",
    "\n",
    "$$\\mathcal{L}_{\\lambda}(\\theta, \\mathcal{D}) = \\mathcal{L}(\\theta, \\mathcal{D}) + \\lambda \\mid\\mid\\theta_w\\mid\\mid^{\\mathbf{2}}_2$$ \n",
    "\n",
    "<p>Next, let us select some hyper-parameters. We will specify the regularisation coefficients that we are experimenting with in an array. If the regularisation coefficient is set 0.0, it means no regularisation is performed and hence, it will act as our baseline.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regularisation coefficients\n",
    "lambda_coef_arr = [0, 0.0003, 0.001, 0.0045]\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Number of epochs in each iteration of the whole training set\n",
    "training_steps = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will construct a neural network with the following specifications:</p>\n",
    "<ul>\n",
    "<li>A fully connected Multi-Layer Perceptron network.</li>\n",
    "<li>The MLP will have 2 hidden layers, each one will have 50 units.</li>\n",
    "<li>Tanh activation function will be used.</li>\n",
    "<li>L2 regularisation will be used.</li>\n",
    "<li>Softmax added on the output layer for classification</li>\n",
    "</ul>\n",
    "<p>In the following cell, we will intialize the network weights and biases. This time we intialise all weights with a Gaussian of zero mean and 0.01 standard deviation and biases with 0.0. The network implemetation is going to be a 2-hidden layer MLP, with 50 neurons in each layer. We will initialise three weight matrices and three bias vectors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer dimensions\n",
    "# Set number of neurons in the first hidden layer\n",
    "hid1 = 50\n",
    "# Set number of neurons in the second hidden layer\n",
    "hid2 = 50\n",
    "\n",
    "# Parameter initialisations\n",
    "W_1 = theano.shared(np.random.normal(loc=0., scale=0.01, size=[DX, hid1]))\n",
    "b_1 = theano.shared(np.zeros(hid1))\n",
    "W_2 = theano.shared(np.random.normal(loc=0., scale=0.01, size=[hid1, hid2]))\n",
    "b_2 = theano.shared(np.zeros(hid2))\n",
    "W_o = theano.shared(np.random.normal(loc=0., scale=0.01, size=[hid2, CL]))\n",
    "b_o = theano.shared(np.zeros(CL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We define the Theano variables for the input, target values and regularisation coefficients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano inputs\n",
    "\n",
    "# Input features\n",
    "X1 = T.dmatrix()\n",
    "# Class labels\n",
    "target = T.ivector()\n",
    "# Regularisation coefficent\n",
    "lambda_coef = T.dscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the following cell, we construct our Theano computational graph. We will calculate the first and second layer activations, predictions for the classifications, cost function, accuracy and gradients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano computational graph\n",
    "\n",
    "# Calculate the first and second hidden layer activations\n",
    "a1 = T.dot(X1, W_1) + b_1\n",
    "h1 = T.tanh(a1)\n",
    "a2 = T.dot(h1, W_2) + b_2\n",
    "h2 = T.tanh(a2)\n",
    "a_out = T.dot(h2, W_o) + b_o\n",
    "\n",
    "# Add softmax function to get the probabilities for each class.\n",
    "y1 = T.nnet.softmax(a_out)\n",
    "\n",
    "# Obtain prediction of correct class\n",
    "y_pred = T.argmax(y1, axis = 1)\n",
    "\n",
    "################# Insert Your Code Here ###################\n",
    "\n",
    "# Implement L2 regularisation. Compute the L2 regularised cost \n",
    "# across all samples. Here, we shall use the categorical\n",
    "# cross-entropy as the cost function.\n",
    "\n",
    "# Compute the un-modified cost.\n",
    "# Hint: Use T.nnet.categorical_crossentropy. Check the \n",
    "# Theano documentation for more information. Refer to the \n",
    "# previous demonstrations to see how the cost is computed.\n",
    "cost_xent =\n",
    "\n",
    "# Compute the L2 norm of all the weights that are to be \n",
    "# optimised. \n",
    "# Hint: We have 3 sets of weights W_1, W_2 and W_o. Refer to \n",
    "# chapter 7, section 7.1.1, page 227 in the Deep Learning \n",
    "# textbook. (online version, 21st Nov 2017)\n",
    "L2_reg =\n",
    "\n",
    "# Add the regularisation to the cost.\n",
    "# Hint: Observe the formula for L2 regularised cost.\n",
    "cost =\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Calculate the accuracy by comparing the predictions to the labels\n",
    "accuracy = T.mean(T.eq(y_pred,target))\n",
    "\n",
    "# Calculate the gradients\n",
    "gW_1 = T.grad(cost, W_1)\n",
    "gb_1 = T.grad(cost, b_1)\n",
    "gW_2 = T.grad(cost, W_2)\n",
    "gb_2 = T.grad(cost, b_2)\n",
    "gW_o = T.grad(cost, W_o)\n",
    "gb_o = T.grad(cost, b_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let us define the training function that is called from the training loop later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training function and parameter updates\n",
    "train = theano.function(\n",
    "    inputs = [X1, target, lambda_coef],\n",
    "    outputs = [y_pred, cost_xent],\n",
    "    updates = [(W_1, W_1 - learning_rate * gW_1), \n",
    "               (b_1, b_1 - learning_rate * gb_1),\n",
    "               (W_2, W_2 - learning_rate * gW_2), \n",
    "               (b_2, b_2 - learning_rate * gb_2),\n",
    "               (W_o, W_o - learning_rate * gW_o), \n",
    "               (b_o, b_o - learning_rate * gb_o)]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we define the Theano functions for predicting the correct class and for calculating the accuracy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano function for predicting classes\n",
    "pred_f = theano.function(inputs = [X1], \n",
    "                             outputs = y_pred)\n",
    "\n",
    "# Theano function for accuracy evaluation\n",
    "accuracy_f = theano.function(inputs = [X1, target], \n",
    "                             outputs = accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the next cell, we initialise some arrays to collect results for visualisation. You can skim through this.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising some numpy arrays for collecting data for visualisations\n",
    "points = int(training_steps/1000)\n",
    "losses_all = np.empty((0, points))\n",
    "accuracy_train_all = np.empty((0, points))\n",
    "accuracy_test_all = np.empty((0, points))\n",
    "train_acc_final = np.array([])\n",
    "test_acc_final = np.array([])\n",
    "\n",
    "# Create a training grid for the contour plot\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid_train = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z_train = np.empty((0, grid_train.shape[0]))\n",
    "\n",
    "# Create a testing grid for the contour plot\n",
    "h = 0.02\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z_test = np.empty((0, grid_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we shall create the main training loop. The training loop is run for each regularisation coefficient in the array.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(lambda_coef_arr)):\n",
    "    losses = np.array([])\n",
    "    accuracy_train = np.array([])\n",
    "    accuracy_test = np.array([])\n",
    "    iteration = np.array([])\n",
    "    lambda_coef = lambda_coef_arr[idx]\n",
    "    print('***** Training model *****')\n",
    "    \n",
    "    for i in range(training_steps):\n",
    "        # Call the previously defined training function.\n",
    "        my_pred, my_cost = train(X, y, lambda_coef)\n",
    "        if i % 1000 == 0:\n",
    "            # Accumulate results for visualisation.\n",
    "            losses = np.append(losses, my_cost)\n",
    "            iteration = np.append(iteration, i)\n",
    "            accuracy_train = np.append(accuracy_train, accuracy_f(X,y))\n",
    "            accuracy_test = np.append(\n",
    "                                      accuracy_test, \n",
    "                                      accuracy_f(X_test,y_test))\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Iteration %d: loss %f\" % (i, my_cost))\n",
    "            print(\"Training accuracy\", accuracy_f(X,y))\n",
    "            print(\"Test accuracy\", accuracy_f(X_test,y_test))\n",
    "            \n",
    "    # Accumulate the results\n",
    "    losses = losses.reshape((1, points))\n",
    "    losses_all = np.append(losses_all, losses, axis = 0)\n",
    "    accuracy_train = accuracy_train.reshape((1, points))\n",
    "    accuracy_train_all = np.append(accuracy_train_all, accuracy_train, axis=0)\n",
    "    accuracy_test = accuracy_test.reshape((1, points))\n",
    "    accuracy_test_all = np.append(accuracy_test_all, accuracy_test, axis=0)\n",
    "    \n",
    "    pred_train = pred_f(grid_train) \n",
    "    pred_train = pred_train.reshape((1, grid_train.shape[0]))\n",
    "    Z_train = np.append(Z_train, pred_train, axis = 0)\n",
    "    \n",
    "    pred_test = pred_f(grid_test) \n",
    "    pred_test = pred_test.reshape((1, grid_test.shape[0]))\n",
    "    Z_test = np.append(Z_test, pred_test, axis = 0)\n",
    "    \n",
    "    train_acc_final = np.append(train_acc_final, accuracy_f(X,y))\n",
    "    test_acc_final = np.append(test_acc_final, accuracy_f(X_test,y_test))\n",
    "    print('***** Training complete *****')\n",
    "    \n",
    "    # Re-initialise the weights and biases        \n",
    "    W_1.set_value(np.random.normal(loc=0., scale=0.01, size =[DX, hid1]))\n",
    "    b_1.set_value(np.zeros(hid1))\n",
    "    W_2.set_value(np.random.normal(loc=0., scale=0.01, size =[hid1, hid2]))\n",
    "    b_2.set_value(np.zeros(hid2))\n",
    "    W_o.set_value(np.random.normal(loc=0., scale=0.01, size =[hid2, CL]))\n",
    "    b_o.set_value(np.zeros(CL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's inspect the final training and test accuracies for each of the regularisation coefficients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(lambda_coef_arr)):\n",
    "    print(\"Regularisation coefficient: %f\" %(lambda_coef_arr[idx]))\n",
    "    print(\"Training Accuracy = %f%%\" %(train_acc_final[idx] * 100.))\n",
    "    print(\"Test Accuracy = %f%% \\n\" %(test_acc_final[idx] * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also observe how the cost varies with different regularisation coefficients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for idx in range(len(lambda_coef_arr)):\n",
    "    plt.plot(iteration, losses_all[idx, :],\n",
    "             label='Reg. Coefficient = ' + str(lambda_coef_arr[idx]))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Variation of Training Losses with Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's visualise how the classifier performs on the training data with a contour plot. Each colour corresponds to a class and depicts the prediction of our classifier. Observe the decision boundaries.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the resulting classifier and the training data\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "fig_tr, ax_tr = plt.subplots(2, 2, figsize=(16, 13))\n",
    "fig_tr.suptitle(\n",
    "    \"Resulting Classifiers on Training Data\")\n",
    "for idx in range(len(lambda_coef_arr)):\n",
    "    col = idx % 2\n",
    "    row = int(idx / 2)\n",
    "    Z = Z_train[idx, :]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax_tr[row][col].contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    ax_tr[row][col].scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=plt.cm.Spectral)\n",
    "    ax_tr[row][col].set_xlim(xx.min(), xx.max())\n",
    "    ax_tr[row][col].set_ylim(yy.min(), yy.max())\n",
    "    ax_tr[row][col].set_title(\n",
    "        'Regularisation Coefficient = ' + str(lambda_coef_arr[idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also compare the performance of each of our classifiers with different regularisation parameters. Below, we plot the variations of both the training accuracies and test accuracies for each classifier.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_acc, ax_acc = plt.subplots(2, 2, figsize=(16, 13))\n",
    "fig_acc.suptitle(\n",
    "    \"Comparison of Training and Test Accuracies\")\n",
    "for idx in range(len(lambda_coef_arr)):\n",
    "    col = idx % 2\n",
    "    row = int(idx / 2)    \n",
    "    ax_acc[row][col].plot(iteration, accuracy_train_all[idx, :],\n",
    "                          label='Training Accuracy')\n",
    "    ax_acc[row][col].plot(iteration, accuracy_test_all[idx, :],\n",
    "                          label='Test Accuracy')\n",
    "    ax_acc[row][col].set_ylim(0.3, 1)\n",
    "    ax_acc[row][col].set_xlabel('Iterations')\n",
    "    ax_acc[row][col].set_ylabel('Training/Test accuracy')\n",
    "    ax_acc[row][col].set_title('Regularisation Coefficient = '\n",
    "                               + str(lambda_coef_arr[idx]))\n",
    "    ax_acc[row][col].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also visualise how the classifier performs on the testing data with a contour plot as before. Each colour corresponds to a class and depicts the prediction of our classifier. Observe these decision boundaries as well.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the resulting classifier and the test data\n",
    "h = 0.02\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "fig_test, ax_test = plt.subplots(2, 2, figsize=(16, 13))\n",
    "fig_test.suptitle(\n",
    "    \"Resulting Classifiers on Testing Data\")\n",
    "for idx in range(len(lambda_coef_arr)):\n",
    "    col = idx % 2\n",
    "    row = int(idx / 2)\n",
    "    Z = Z_test[idx, :]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax_test[row][col].contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    ax_test[row][col].scatter(X_test[:, 0], X_test[:, 1], c=y_test,\n",
    "                              s=10, cmap=plt.cm.Spectral)\n",
    "    ax_test[row][col].set_xlim(xx.min(), xx.max())\n",
    "    ax_test[row][col].set_ylim(yy.min(), yy.max())\n",
    "    ax_test[row][col].set_title('Regularisation Coefficient = '\n",
    "                                +str(lambda_coef_arr[idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with some key observations and discussion. What are your comments about the results? Do you have any observations on the cost and accuracy plot behaviour? What are your comments on the decision boundaries? What would be the appropriate regularisation parameter to choose?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
