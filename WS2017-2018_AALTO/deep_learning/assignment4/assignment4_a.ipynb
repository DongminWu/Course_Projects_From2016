{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with your name and student number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pagebreak$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Exercise on Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Recurrent Neural Networks (RNN) are popular models that are used frequently, for example, in Natural Language Processing (NLP). Usually, in neural networks we assume that all inputs (and outputs) are independent of each other. In RNNs, the output is dependent on the previous computations. An interesting way to think of RNNs is that it has \"memory\" that captures information about what has been calculated so far. In this exercise, you will implement a simple Recurrent Neural Network similar to the image below and train it on the MNIST dataset to perform classification on the MNIST digits. Remember that mini batches will be used.</p>\n",
    "<p>This part has a maximum points of 4. The exercise on regularisation in the other assignment notebook will have a maximum of 2 points.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/D43uEDO.png\" alt=\"RNN Architecture\" title=\"Figure 1\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the image above, note the following things:</p>\n",
    "<ul>\n",
    "<li>$x_t$ are the pixels at time step $t$.</li>\n",
    "<li>At each time step, we will feed in one row of the MNIST digit.</li>\n",
    "<li>There are totally 28 time steps.</li>\n",
    "<li>The hidden states are 100-dimensional. $h_t$ is the set hidden state at time step $t$.</li>\n",
    "<li>$\\mathcal{U}$, $\\mathcal{V}$ and $\\mathcal{W}$ are the weights that will be learnt and are shared across all the time steps. This is unlike standard MLP neural networks that use separate parameters at each layer.</li>\n",
    "<li>We compute the output by averaging over all the hidden states.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will begin by first importing the necessary python libraries:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "\n",
    "print('***** Import complete *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's load the dataset. We will be using the same MNIST Handwritten digit dataset as before.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file and load the training, validation and\n",
    "# test sets.\n",
    "with gzip.open('./mnist.pkl.gz', 'rb') as f:\n",
    "    try:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    except:\n",
    "        train_set, valid_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The MNIST image features are of dimensions (number_of_samples x 784). Let's reshape it into (number_of_samples x 28 x 28) and obtain the feature inputs and labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = (train_set[0].reshape(-1, 28, 28), train_set[1])\n",
    "valid_set = (valid_set[0].reshape(-1, 28, 28), valid_set[1])\n",
    "test_set = (test_set[0].reshape(-1, 28, 28), test_set[1])\n",
    "\n",
    "# Obtain the training, test and validation input features with labels\n",
    "# X_train - (50000 x 28 x 28)\n",
    "# Y_train - (50000 x 1)\n",
    "(X_train, Y_train) = train_set\n",
    "\n",
    "# X_valid - (10000 x 28 x 28)\n",
    "# Y_valid - (10000 x 1)\n",
    "(X_valid, Y_valid) = valid_set\n",
    "\n",
    "# X_test - (10000 x 28 x 28)\n",
    "# Y_test - (10000 x 1)\n",
    "(X_test, Y_test) = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It is good practice to always shuffle your training set. Below, we do this by shuffling the indices of the training set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "X_train = X_train[permutation]\n",
    "Y_train = Y_train[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The model that we will implement, requires the class labels to be \"one-hot\" encoded. One-hot encoding is encoding of categorical variables in a 1 of K scheme (K being the number of classes). For example, if the digit label is 2, it would be encoded as [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating copies of the original labels\n",
    "Y_labels = Y_train\n",
    "Yv_labels = Y_valid\n",
    "Yt_labels = Y_test\n",
    "\n",
    "def onehot(input):\n",
    "    # Function that performs one-hot encoding\n",
    "    \n",
    "    # Inputs:\n",
    "    # input - array of class labels\n",
    "    \n",
    "    # Outputs:\n",
    "    # onehot - array of encoded labels\n",
    "    \n",
    "    ################# Insert Your Code Here ###################\n",
    "    \n",
    "    # Implement one-hot encoding on the class labels. Store \n",
    "    # this is in a multi-dimensional array.\n",
    "    \n",
    "    # Hint: input is of dimensions num_samples x 1. The output, 'onehot'\n",
    "    # is of dimension num_samples x 10. If the digit label is [3]\n",
    "    # the equivalent one-hot encoding is [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "# Perform One Hot encoding on the training, test\n",
    "# and validation labels\n",
    "Y_train = onehot(Y_train)\n",
    "Y_valid = onehot(Y_valid)\n",
    "Y_test  = onehot(Y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Each step of our RNN will have 100-dimensional hidden state (refer to the figure above). These hidden states will act as the network's \"memory\". At each step, the current value of the hidden state is calculated based on its value in the previous step. Each sample comprises of features of dimension 28 x 28. Therefore, the number of inputs as well as time steps would be 28. Below, we initialise some of the hyper-parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set number of iterations\n",
    "iterations = 1000\n",
    "\n",
    "# Set number of input pixels or in other\n",
    "# words, the number of horizontal pixels.\n",
    "input_d = 28\n",
    "\n",
    "# Set number of hidden units.\n",
    "hidden_d = 100\n",
    "\n",
    "# Set number of outputs.\n",
    "output_d = 10\n",
    "\n",
    "# Set mini-batch size.\n",
    "mini_batch_size = 100\n",
    "\n",
    "# Set the number of time steps or in other words the\n",
    "# number of vertical input pixels.\n",
    "time_steps = 28\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Set the seed for consistency\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We now initialise all the parameters that will be learnt/optimised. Remember that in RNNs these parameters are shared among all the steps.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the shared weights\n",
    "# Set the random state\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "################# Insert Your Code Here ###################\n",
    "\n",
    "# Initialise the weights U, V and W. Use the Glorot and Bengio (2010), \n",
    "# normalized initialisation. \n",
    "\n",
    "# Hint: The dimensions of U will be (input_d x hidden_d), \n",
    "# W will be (hidden_d x hidden_d) and V will be (hidden_d x output_d).\n",
    "# For the Glorot and Bengio initialisation refer to chapter 8, page 299,\n",
    "# equation 8.23 in the Deep Learning textbook\n",
    "# (online version, 21st Nov 2017).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "U = \n",
    "\n",
    "\n",
    "\n",
    "W = \n",
    "\n",
    "\n",
    "\n",
    "V =   \n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Initialise the hidden unit bias\n",
    "b_hidden = theano.shared(np.zeros((hidden_d,)))\n",
    "\n",
    "# Initialise the final output bias\n",
    "b_V = theano.shared(np.zeros((output_d,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create the Theano data structures for the features and class labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 3D tensor for features\n",
    "x = T.tensor3('x')\n",
    "\n",
    "# Create matrix of one-hot encoded class labels\n",
    "y = T.matrix('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this architecture, we use the average of all hidden states instead of using only the hidden state from the last step for performing the prediction. We create a tensor to store the hidden states ($\\mathbf{h_t}$) at each time step. We are using mini-batches and have 100 hidden units. For this purpose, we need to create a tensor to accumulate the hidden state values from each time step.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a tensor for the hidden states of \n",
    "# dimensions: (mini_batch_size x 100)\n",
    "h_states = T.zeros((x.shape[0], hidden_d))\n",
    "\n",
    "# Create a tensor to accumulate the  hidden \n",
    "# states of dimensions: (mini_batch_size x 100)\n",
    "h_states_sum = T.zeros((x.shape[0], hidden_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>At each time step $t$, we must compute the value of the hidden states, $\\mathbf{h_t}$. We compute $\\mathbf{h_t}$ based on the previous hidden state, $\\mathbf{h_{t-1}}$ and the input at the current step, $\\mathbf{x_t}$ using the formula: $\\mathbf{h_t} = tanh((\\mathcal{U}\\mathbf{x_t} + \\mathcal{W}\\mathbf{h_{t-1}})+biases)$. This is the general form of the formula. Do not forget to keep track of the dimensions of the tensors in your implementation. We accumulate the value of the hidden states at each time step so that we can compute the mean value of the hidden states later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(time_steps):\n",
    "    \n",
    "    ################# Insert Your Code Here ###################\n",
    "\n",
    "    # Obtain the current value of the hidden state by completing the\n",
    "    # code below. \n",
    "    # For this, compute the dot products on one row of features of each\n",
    "    # sample in the mini-batch and apply the non-linearity Tanh.\n",
    "    \n",
    "    # Hint: Observe the formula for the current value of the hidden\n",
    "    # state carefully. Remember that we already have a 3 dimension\n",
    "    # tensor for x which contains the training features for a \n",
    "    # mini-batch. You could access the row associated with a \n",
    "    # time-step across all samples of the mini-batch using x[:, t, :].\n",
    "    # We already have U, V, W, b_hidden and h_states (which has the \n",
    "    # hidden state from the previous time step).\n",
    "    \n",
    "    a = \n",
    "    \n",
    "    # Apply a non-linearity\n",
    "    h_states = T.tanh(a)\n",
    "    \n",
    "    # Accumulate the hidden state values\n",
    "    \n",
    "    # Hint: Save the sum of the hidden states up to the current\n",
    "    # time step.\n",
    "    h_states_sum = \n",
    "    \n",
    "    ##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, we shall compute the final class predictions on each of the samples in the mini-batch. To do this, we average over all the values of the hidden states in each time step (However, we could have also used just the value of the final hidden state. This may not perform as well as using the mean). The final predictions can be written as: $y_{prob} = softmax((\\mathcal{V} \\cdot \\frac{1}{\\textit{timesteps}}\\Sigma_{t=1}^{\\textit{timesteps}}\\mathbf{h_t}) + biases))$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################# Insert Your Code Here ###################\n",
    "\n",
    "# Observe the above formula for finding the final prediction \n",
    "# probabilities for each class and complete the code below.\n",
    "\n",
    "# Compute the mean of the hidden states\n",
    "# Hint: We already calculated the sum of the hidden states\n",
    "# above.\n",
    "h_states_mean = \n",
    "\n",
    "# Compute the dot product and add the bias\n",
    "# Hint: We calculated h_states_mean above. Now, observe\n",
    "# the formula again. Compute the dot product with the\n",
    "# corresponding weight. Do not forget to add the bias b_V!\n",
    "y_hat = \n",
    "\n",
    "# Apply the softmax function to get 10 outputs per sample\n",
    "# with values ranging from 0 to 1.\n",
    "# Hint: All we have left is to pass the above computation \n",
    "# into the softmax function.\n",
    "y_prob = \n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Choose the class with the highest probability\n",
    "y_pred = T.argmax(y_prob, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Stochastic Gradient Descent will be used to train the parameters. We must define a cost function that is optimised. The Categorical Cross-Entropy will be used as the cost.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################# Insert Your Code Here ###################\n",
    "\n",
    "# Compute the cost across all samples in the mini-batch. \n",
    "# Here we shall use the categorical cross-entropy.\n",
    "\n",
    "# Hint: Use T.nnet.categorical_crossentropy. Check the \n",
    "# Theano documentation for more information. Refer to the \n",
    "# previous demonstrations to see how the cost is computed.\n",
    "cost = \n",
    "\n",
    "\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Compute the gradients for each of the parameters as before:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_W = T.grad(cost, W)\n",
    "grad_U = T.grad(cost, U)\n",
    "grad_V = T.grad(cost, V)\n",
    "grad_bh = T.grad(cost, b_hidden)\n",
    "grad_bV = T.grad(cost, b_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We shall define Theano functions to train and test our RNN. We use the same testing function to test on validation and testing datasets. Note: Theano will now construct the computational graph, which is quite deep. It might take a minute or two.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a Theano function to test our RNN\n",
    "cost_pred = theano.function(\n",
    "    inputs = [x, y],\n",
    "    outputs = [cost, y_pred])\n",
    "\n",
    "################# Insert Your Code Here ###################\n",
    "\n",
    "# Update all the parameters. Create a list with the updated\n",
    "# values of the parameters W, U, V, b_hidden, b_V. This list\n",
    "# would be passed into our training function.\n",
    "\n",
    "# Hint: This step is similar to what we did in previous \n",
    "# exercises and demos. You can refer to them to understand\n",
    "# how to perform the updates. But keep in mind the parameters\n",
    "# that we are updating in this model are different!\n",
    "updates = [] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Define a Theano function to train our RNN\n",
    "sgd_step = theano.function(\n",
    "    inputs = [x, y],\n",
    "    outputs = [cost, y_pred],\n",
    "    updates = updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, we can define our main training loop. In each iteration, we create a mini-batch of the specified size by drawing random training samples from our entire training set. We then perform one-hot encoding on the class labels and perform training of our RNN on the mini-batch.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create arrays to save costs and accuracies \n",
    "# for plotting\n",
    "cost_valid_arr        = np.array([])\n",
    "accuracy_valid_arr    = np.array([])\n",
    "cost_training_arr     = np.array([])\n",
    "accuracy_training_arr = np.array([])\n",
    "\n",
    "print('***** Training model *****')\n",
    "# Create the main training loop\n",
    "for i in range(iterations):\n",
    "    \n",
    "    in_1 = np.zeros((mini_batch_size, 28, 28))\n",
    "    y_onehot = np.zeros((mini_batch_size, 10), dtype='int32')\n",
    "    y_lab = np.array([])\n",
    "    \n",
    "    # Create the mini-batch by drawing from the entire\n",
    "    # training set.\n",
    "    for j in range(mini_batch_size):\n",
    "        # Choose a random sample to include\n",
    "        ix = np.random.randint(0,50000)\n",
    "        \n",
    "        # Obtain the training features\n",
    "        in_1[j] = X_train[ix]\n",
    "        \n",
    "        # Obtain the one hot encoded labels\n",
    "        y_onehot[j] = Y_train[ix]\n",
    "        \n",
    "        # Obtain the original call label\n",
    "        y_lab = np.append(y_lab, Y_labels[ix])\n",
    "    \n",
    "    # Perform training. Obtain training cost and predictions\n",
    "    train_cost, train_pred = sgd_step(in_1,y_onehot)\n",
    "    \n",
    "    # Obtain validation cost and predictions\n",
    "    valid_cost, valid_pred = cost_pred(X_valid[0:100], Y_valid[0:100])\n",
    "        \n",
    "    # Compute training accuracy\n",
    "    train_accuracy = np.mean(y_lab == train_pred)\n",
    "    \n",
    "    # Obtain validation accuracy\n",
    "    valid_accuracy = np.mean(Yv_labels[0:100] == valid_pred)\n",
    "    \n",
    "    # Saving cost and accuracies\n",
    "    cost_training_arr     = np.append(cost_training_arr, train_cost)\n",
    "    accuracy_training_arr = np.append(accuracy_training_arr, train_accuracy)\n",
    "    cost_valid_arr        = np.append(cost_valid_arr, valid_cost)\n",
    "    accuracy_valid_arr    = np.append(accuracy_valid_arr, valid_accuracy)\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print('Iteration:           %d' % (i))\n",
    "        print('Training cost:       %f' % (train_cost))\n",
    "        print('Validation cost:     %f' % (valid_cost))\n",
    "        print('Training accuracy:   %f' % (train_accuracy))\n",
    "        print('Validation accuracy: %f' % (valid_accuracy))\n",
    "print('***** Training complete *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can now generate some plots to see how the cost and accuracy varies as training progresses.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_tr, ax_tr = plt.subplots(1, 2, figsize=(16, 4))\n",
    "fig_tr.suptitle(\n",
    "    \"Cost and Accuracy on Training Set\")\n",
    "ax_tr[0].plot(cost_training_arr, '-r')\n",
    "ax_tr[0].set_xlabel('Iterations')\n",
    "ax_tr[0].set_ylabel('Cost')\n",
    "ax_tr[1].plot(accuracy_training_arr, '-g')\n",
    "ax_tr[1].set_xlabel('Iterations')\n",
    "ax_tr[1].set_ylabel('Accuracy')\n",
    "fig_tr.subplots_adjust(wspace=.4)\n",
    "plt.show()\n",
    "\n",
    "fig_v, ax_v = plt.subplots(1, 2, figsize=(16, 4))\n",
    "fig_v.suptitle(\n",
    "    \"Cost and Accuracy on Validation Set\")\n",
    "ax_v[0].plot(cost_valid_arr, '-r')\n",
    "ax_v[0].set_xlabel('Iterations')\n",
    "ax_v[0].set_ylabel('Cost')\n",
    "ax_v[1].plot(accuracy_valid_arr, '-g')\n",
    "ax_v[1].set_xlabel('Iterations')\n",
    "ax_v[1].set_ylabel('Accuracy')\n",
    "fig_v.subplots_adjust(wspace=.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After the training is complete, we can compute the prediction accuracy of our RNN on the entire test set. This will give us an idea of how our model will perform on unseen data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain testing cost and predictions\n",
    "test_cost, test_pred = cost_pred(X_test, Y_test)\n",
    "\n",
    "# Compute testing accuracy\n",
    "test_accuracy = np.mean(Yt_labels == test_pred)\n",
    "\n",
    "print('Testing accuracy: %f %%' % (test_accuracy * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with some key observations and discussion. How suitable are recurrent neural networks for this kind of task? What are your comments about the results? Do you have any observations on the cost and accuracy plot behaviour? What different options could there be to possibly improve the result?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
