{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with your name and student number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pagebreak$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise on Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this exercise you will extend and improve the accuracy of the Convolution Neural Network discussed in the demonstration by doing the following things: </p>\n",
    "<ul>\n",
    "<li>Creating a deep Convolutional Neural Network.</li>\n",
    "<li>Tweaking the hyper-parameters such as the number of channels in each layer, activation functions, mini batch size, etc;</li>\n",
    "</ul>\n",
    "<p>The architecture of the network that we will create is similar to the figure below (This figure will not be displayed when exported to .pdf):</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/AjQYgPv.png\" alt=\"Demo Architecture\" title=\"Figure 1\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will begin by first importing the necessary python libraries:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "from exercise_helper import load_data, pooling, convLayer\n",
    "from exercise_helper import fullyConnectedLayer\n",
    "from exercise_helper import negative_log_lik, errors\n",
    "from exercise_helper import generate_plot\n",
    "%matplotlib inline\n",
    "# Setting the random number generator\n",
    "rng = numpy.random.RandomState(23455)\n",
    "print('***** Import complete *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Note: If the import of 'exercise_helper' fails, ensure that 'exercise_helper.py' is in the same folder as this notebook.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can create the Theano Computation Graph. We shall partition the data into mini-batches and then create a computation graph for training, validation and testing. The overall logic is very similar to the demonstration. Refer to the demonstration for more details.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_conv_net(learning_rate, num_epochs,\n",
    "                        num_filters, mini_batch_size, activation):\n",
    "    # Function to create the convolutional neural network, train and\n",
    "    # evaluate it. This function must be called to run the network.\n",
    "    \n",
    "    # Inputs:\n",
    "    # learning_rate - Learning rate for Stochastic Gradient Descent\n",
    "    # num_epochs - Number of training epochs\n",
    "    # num_filters - Number of kernels for each convolution layer\n",
    "    #               for e.g. 2 layers - [20, 50]. \n",
    "    #.              layer1 = 20, layer2 = 50\n",
    "    # mini_batch_size - Mini-batch size to be used\n",
    "    # activation - Activation function to use\n",
    "    \n",
    "    # Outputs:\n",
    "    # Plot of the cost, prediction errors on validation set and\n",
    "    # visualisation of weights of the first convolutional layer\n",
    "    \n",
    "    # Partitioning into mini- batches\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches //= mini_batch_size\n",
    "    n_valid_batches //= mini_batch_size\n",
    "    n_test_batches //= mini_batch_size\n",
    "\n",
    "    print('train: %d batches, test: %d batches,'\n",
    "          ' validation: %d batches'\n",
    "          % (n_train_batches, n_test_batches, n_valid_batches))\n",
    "    \n",
    "    mb_index = T.lscalar() # mini-batch index\n",
    "    x = T.matrix('x') # rasterised images\n",
    "    y = T.ivector('y') # image labels\n",
    "    layer_weights = [];\n",
    "    print('***** Constructing model ***** ')\n",
    "\n",
    "    # Reshaping matrix of mini_batch_size set of images into a \n",
    "    # 4-D tensor \n",
    "    layer0_input = x.reshape((mini_batch_size, 1, 28, 28))\n",
    "\n",
    "    ################# Insert Your Code Here ###################\n",
    "\n",
    "    # Construct first convolution and pooling layer\n",
    "    # Hint: Use the convLayer function. See demonstration.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Construct second convolution and pooling layer\n",
    "    # Hint: Use the convLayer function. See demonstration.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Classify the values using the fully-connected\n",
    "    # activation layer.\n",
    "    # Hint: Remember to flatten the output from the \n",
    "    # convolutional layer. Use the fullyConnectedLayer function.\n",
    "    # See demonstration.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################################\n",
    "\n",
    "    # Cost that is minimised during stochastic descent.\n",
    "    cost = negative_log_lik(y=y, p_y_given_x=p_y_given_x)\n",
    "\n",
    "    # Creating a function that computes the mistakes on the test set\n",
    "    # mb_index is the mini_batch_index\n",
    "    test_model = theano.function(\n",
    "        [mb_index],\n",
    "        errors(y, y_pred),\n",
    "        givens={\n",
    "            x: test_set_x[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size],\n",
    "            y: test_set_y[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size]})\n",
    "\n",
    "    # Creating a function that computes the mistakes on the validation\n",
    "    # set\n",
    "    valid_model = theano.function(\n",
    "        [mb_index],\n",
    "        errors(y, y_pred),\n",
    "        givens={\n",
    "            x: valid_set_x[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size],\n",
    "            y: valid_set_y[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size]})\n",
    "    \n",
    "    ################# Insert Your Code Here ###################\n",
    "\n",
    "    # Create list of parameters to fit during training.\n",
    "    # Hint: Include the parameters from the two convolution layers\n",
    "    # and activation layer\n",
    "    \n",
    "    params = \n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    # Creating a list of gradients\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # Creating a function that updates the model parameters by SGD.\n",
    "    # The updates list is created by looping over all \n",
    "    # params[i], grads[i]) pairs.\n",
    "    updates = [(param_i, param_i - learning_rate * grad_i)\n",
    "               for param_i, grad_i in zip(params, grads)]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [mb_index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size],\n",
    "            y: train_set_y[\n",
    "                mb_index * mini_batch_size:\n",
    "                (mb_index + 1) * mini_batch_size]})\n",
    "\n",
    "    epoch = 0\n",
    "    cost_arr = numpy.array([])\n",
    "    valid_score_arr = numpy.array([])\n",
    "    valid_score_arr = numpy.append(valid_score_arr, 1)\n",
    "\n",
    "    print('***** Training model *****')\n",
    "    if (num_epochs < 1):\n",
    "        print(\"Too few epochs!\")\n",
    "        return\n",
    "    while (epoch < num_epochs):\n",
    "        epoch = epoch + 1\n",
    "        print(\"Training in epoch: %d / %d\" % (epoch, num_epochs),\n",
    "              end='\\r')\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            # Computing number of iterations performed or total number\n",
    "            # of mini-batches executed.\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            # cost of each minibatch\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "            cost_arr = numpy.append(cost_arr, cost_ij)\n",
    "\n",
    "        # Computing loss on each validation mini-batch after each epoch\n",
    "        valid_losses = [valid_model(i) for i in range(n_valid_batches)]\n",
    "        valid_score_arr = numpy.append(\n",
    "                                       valid_score_arr,\n",
    "                                        numpy.mean(valid_losses))\n",
    "    print('***** Training Complete *****')\n",
    "    # Computing mean error rate on test set\n",
    "    test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "    test_score = numpy.mean(test_losses)\n",
    "    print('Prediction error: %f %%' % (test_score * 100.))\n",
    "    # Generating the plots\n",
    "    generate_plot(cost_arr, range(1, iter+2),\n",
    "                  valid_score_arr,\n",
    "                  range(0, epoch+1),\n",
    "                  layer0_params[0].get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we shall define some hyper-parameters and evaluate the model. We will compute the final Prediction Error on the test set. To complete this assignment, you must do the following things:</p>\n",
    "<ol>\n",
    "<li>Set your parameters in the cell below</li>\n",
    "<li>Run the experiment. Then, describe and discuss it in the 'Conclusions' cell below. You must do this for each experiment that you run.</li>\n",
    "<li>Especially in CSC notebooks, do not forget to restart the kernel after each experiment.</li>\n",
    "<li>Repeat from step 1 and perform at least 4 experiments.</li>\n",
    "<li>Run the experiment with the best result again to display the plots in the final submission.</li>\n",
    "<li>Download the completed assignment as PDF and submit as usual. The final PDF will contain the plots from only your best experiment as well as the discussion of all your experiments in the 'Conclusions' cell.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################# Insert Your Code Here ###################\n",
    "\n",
    "# Description and examples of parameters to train_test_conv_net()\n",
    "\n",
    "# learning_rate - Sets the learning rate for Stochastic Gradient\n",
    "#                 Descent. e.g.: 0.1\n",
    "# num_epochs - Sets the number of training epochs. E.g.: 10\n",
    "# num_filters - Sets the number of kernels for each \n",
    "#               convolution layer. for e.g. 2 layers: [4, 8]\n",
    "#               implies layer1 = 4, layer2 = 8\n",
    "# mini_batch_size - Sets the mini-batch size to be used in \n",
    "#                   the experiment. E.g: 50\n",
    "# activation - Sets the activation function to be used.\n",
    "#              E.g.: T.tanh\n",
    "# train_size - Sets the number of training samples to be used.\n",
    "#              E.g. 6000. \n",
    "\n",
    "# Note: The Kernel may crash on too large values of the \n",
    "# num_epochs, num_filters, mini_batch_size and train_size due to \n",
    "# memory limitations. This may happen especially on CSC notebooks\n",
    "# as it is a shared resource.\n",
    "# If that happens use smaller values!\n",
    "\n",
    "learning_rate   =\n",
    "num_epochs      = \n",
    "num_filters     = [ ]\n",
    "mini_batch_size =\n",
    "activation      =\n",
    "train_size      =\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# Loading dataset\n",
    "# We use only a subset of the full dataset.\n",
    "# Validation and test sets will be 1/3 of train \n",
    "# set size\n",
    "datasets = load_data('mnist.pkl.gz', train_size)\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "print('Training set: %d samples'\n",
    "      %(train_set_x.get_value(borrow=True).shape[0])) \n",
    "print('Test set: %d samples'\n",
    "      %(test_set_x.get_value(borrow=True).shape[0]))\n",
    "print('Validation set: %d samples'\n",
    "      %(valid_set_x.get_value(borrow=True).shape[0]))\n",
    "\n",
    "# Beginning the training process\n",
    "train_test_conv_net(learning_rate, num_epochs,\n",
    "                    num_filters, mini_batch_size, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>Now go ahead and experiment with different hyper-parameters such as different number of filters, different number of convolution layers, activation functions etc;. Try to find out which configuration gives the best accuracy.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with the details and discussions on the individual experiments that you performed and final conclusions on them all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
