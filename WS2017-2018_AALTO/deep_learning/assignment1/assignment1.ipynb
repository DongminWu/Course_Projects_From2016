{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your name and student number here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pagebreak$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home assignment - MLP for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following task you will be applying multi-layer perceptron approach for classification problem. The objective would be to become familiar with shallow (1 hidden layer) and simple deep (2 hidden layers) neural network architectures and their implementation in Theano.\n",
    "\n",
    "Code snippets from the demo session has already been provided in the cells below. Modify/add your code wherever specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture : **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the task, you will be exploring architectures with 1 & 2 hidden layers. The input layer will be of 2 dimensions as per the number of training features. Each input dimension is given a weighted connection to every neuron in the first hidden layer. Each of these neurons can be connected further to successive layers in case of multiple hidden layer architectures. Sigmoid activation function with an associated bias term is used for each of these hidden layer neurons.\n",
    "\n",
    "Finally, a sigmoidal function is used in the output-layer to combine activations from last hidden layer neurons and scale it between 0 to 1.\n",
    "\n",
    "This exercise deals with two-class classification problem. Since there are two output classes, NN can be designed to have either a single output neuron (output 0 and 1 indicating each of the classes respectively) or with two output neurons (one for each class). In this exercise, consider single output neuron approach as shown in the diagram below.\n",
    "\n",
    "Use gradient-descent as back-propagation's optimization algorithm and update parameters accordingly. \n",
    "\n",
    "The below diagram shows architecture for a network having two hidden layers with 5 & 3 neurons respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='files/arch2.jpg', height=150, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost function :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consider cross-entropy cost for the classification\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{cost} &= - \\frac{1}{N} \\sum_{i = 1} ^N \\left(\\text{y}_i \\cdot \\log(\\text{ypred}_i)+(1-\\text{y}_i) \\cdot \\log(1- \\text{ypred}_i)\\right)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an in-built implementation theano.tensor.nnet.binary_crossentropy for the above cost function. Please note the order in which predicted and actual values are passed. You can use the same in this exercise. \n",
    "\n",
    "\n",
    "http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#theano.tensor.nnet.nnet.binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Make necessary changes and implement single-hidden layer network for classification**\n",
    "- modify code to consider 2 dimensional inputs\n",
    "- change output layer to sigmoid\n",
    "- implement cross-entropy cost function\n",
    "\n",
    "Some initial code has been provided as part of 'ClassificationSingleHiddenLayerNN' function\n",
    "\n",
    "\n",
    "**2) Experiment by running the single hidden layer network with following cases :**\n",
    "- number of hidden layer neurons = 3, 5 and 8\n",
    "     \n",
    "Code for the function calls with above parameter values has already been provided. \n",
    "Run the code and make sure that the plots make sense. \n",
    "Note that learning rate and number of iterations have been fixed to values 0.2 and 50000 respectively. You might want to reduce the number of iterations for testing purpose, but remember to include results and comments for full 50000 iterations in the final report\n",
    "         \n",
    "**3) Extend the implementation for 2-hidden layer network **\n",
    "\n",
    "A sample architecture has been provided for your reference above and also some initial code has been provided as part of 'ClassificationTwoHiddenLayerNN' function\n",
    "     \n",
    "**4) Run experiments for two-hidden layer network with following cases : **\n",
    "- 3-2 network and 5-3 network and 10-10 network\n",
    "(m-n denotes m & n neurons in hidden layers 1 & 2 respectively)\n",
    "         \n",
    "Code for the functions calls with above parameter values has already been provided. \n",
    "Make sure that respective plots are in-place\n",
    "        \n",
    "**5) Provide brief comments and discuss on the results obtained in the above experiments (max 200 words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half-moon dataset is a 2-dimensional toy dataset with two target classes. It has been synthesized from python package scikit-learn. The below section includes the scatter plot of the data. Note the non-linearity of the class separation boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test datasets\n",
    "# the following data has been generated using scikit-learn\n",
    "Xtrain = np.loadtxt(\"files/halfmoon_Xtrain.txt\") \n",
    "Ytrain = np.loadtxt(\"files/halfmoon_Ytrain.txt\") \n",
    "Xtest = np.loadtxt(\"files/halfmoon_Xtest.txt\") \n",
    "Ytest = np.loadtxt(\"files/halfmoon_Ytest.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise dataset\n",
    "plt.scatter(\n",
    "    Xtrain[Ytrain == 1, 0],\n",
    "    Xtrain[Ytrain == 1, 1],\n",
    "    c=\"#ff9900\",\n",
    "    label=\"class 1\",\n",
    "    s=20,\n",
    "    alpha=0.7)\n",
    "\n",
    "plt.scatter(\n",
    "    Xtrain[Ytrain == 0, 0],\n",
    "    Xtrain[Ytrain == 0, 1],\n",
    "    c=\"#02275a\",\n",
    "    label=\"class 0\",\n",
    "    s=20,\n",
    "    alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.title(\"half-moon data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provides plotting functionality for the classification model. You can just run this cell and move to the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodelfit(Xtrain,\n",
    "                 Ytrain,\n",
    "                 pred_train,\n",
    "                 nnact,\n",
    "                 X1grid,\n",
    "                 X2grid,\n",
    "                 pred_grid,\n",
    "                 cost_train_vec,\n",
    "                 cost_test_vec):\n",
    "    '''method\n",
    "    Inputs :\n",
    "    Xtrain, Ytrain        : N x D, N x 1 : traning datasets\n",
    "    X1grid, X2grid        : G x G, G x G : grid locations as test dataset\n",
    "    pred_train, pred_grid : N x 1, G x 1: model predictions on training dataset and grid dataset\n",
    "    cost_train, cost_test   : num_iter x 1, num_iter x 1 : error across iterations on training and test set\n",
    "    nnact                 : list of activation values in the hidden layer\n",
    "    '''\n",
    "\n",
    "    mt.rcParams['figure.figsize'] = (8, 6)\n",
    "    norm = mt.colors.Normalize(vmin=0., vmax=1.)\n",
    "\n",
    "    nh = [f.shape[1] for f in nnact]\n",
    "    nhiddenl = len(nh)\n",
    "\n",
    "    fig = plt.figure(num=122)\n",
    "\n",
    "    # gs for main plot\n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs00 = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=gs0[0, 0])\n",
    "\n",
    "    # gs for hidden layers\n",
    "    gs1 = gridspec.GridSpecFromSubplotSpec(1, nhiddenl, subplot_spec=gs0[0, 1])\n",
    "\n",
    "    subgs = []\n",
    "    for i in np.arange(nhiddenl):\n",
    "        subgs.append(\n",
    "            gridspec.GridSpecFromSubplotSpec(nh[i], 1, subplot_spec=gs1[0, i]))\n",
    "\n",
    "    # ax for main\n",
    "    ax_00 = fig.add_subplot(gs00[0, 0])  #, adjustable='box-forced'\n",
    "    ax_00.scatter(\n",
    "        Xtrain[Ytrain == 1, 0],\n",
    "        Xtrain[Ytrain == 1, 1],\n",
    "        c=\"#ff9900\",\n",
    "        label=\"class 1\",\n",
    "        s=15,\n",
    "        alpha=0.8)\n",
    "    ax_00.scatter(\n",
    "        Xtrain[Ytrain == 0, 0],\n",
    "        Xtrain[Ytrain == 0, 1],\n",
    "        c=\"#02275a\",\n",
    "        label=\"class 0\",\n",
    "        s=15,\n",
    "        alpha=0.8)\n",
    "    ax_00.contourf(X1grid, X2grid, pred_grid, alpha=0.3)\n",
    "    ax_00.legend()\n",
    "    ax_00.set_title(\"model fit\")\n",
    "\n",
    "    ax_01 = fig.add_subplot(gs00[1, 0])  #, adjustable='box-forced'\n",
    "    ax_01.plot(\n",
    "        np.arange(cost_train_vec.shape[0]),\n",
    "        cost_train_vec,\n",
    "        c=\"#27ae61\",\n",
    "        label=\"train\")\n",
    "    ax_01.plot(\n",
    "        np.arange(cost_test_vec.shape[0]),\n",
    "        cost_test_vec,\n",
    "        c=\"#c1392b\",\n",
    "        label=\"test\")\n",
    "    ax_01.set_xlabel(\"iterations\")\n",
    "    ax_01.set_ylabel(\"cost function\")\n",
    "    ax_01.set_title(\"cost function across iterations\")\n",
    "    ax_01.legend()\n",
    "\n",
    "    axhl = []\n",
    "\n",
    "    # nested list for hidden layer activations\n",
    "    for hlayer in np.arange(nhiddenl):\n",
    "        axnn = []\n",
    "        for hnn in np.arange(nh[hlayer]):\n",
    "            ax = fig.add_subplot(subgs[hlayer][hnn, 0], aspect='equal')\n",
    "            ax.scatter(\n",
    "                Xtrain[:, 0],\n",
    "                Xtrain[:, 1],\n",
    "                c=nnact[hlayer][:, hnn],\n",
    "                cmap=\"RdBu\",\n",
    "                s=5,\n",
    "                norm=norm)\n",
    "            ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "            if hnn == 0:\n",
    "                ax.set_title(\"activations \" + \"\\n\" + \"in layer \" +\n",
    "                             str(hlayer + 1))\n",
    "            axnn.append(ax)\n",
    "        axhl.append(axnn)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 01. Implementation with single hidden layer **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD YOUR CODE IN PLACE OF **#-------------#**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassificationSingleHiddenLayerNN(Xtrain,\n",
    "                                     Ytrain,\n",
    "                                     Xtest,\n",
    "                                     Ytest,\n",
    "                                     nn1=4,\n",
    "                                     training_steps=50000,\n",
    "                                     alpha=0.2):\n",
    "    '''\n",
    "    Input:\n",
    "    Xtrain   : N x D    : traning set features\n",
    "    Ytrian   : N x 1    : training set target\n",
    "    Xtest    : M x D    : test set feaures\n",
    "    Ytest    : M x 1    : test set target\n",
    "    nn1      : scalar   : no. of neurons to be used in first hidden layer\n",
    "    training_steps : scalar : no. of training iteration steps\n",
    "    alpha    : scalar   : learning rate\n",
    "    '''\n",
    "    print(\"*** running ***\")\n",
    "    \n",
    "    # define input and output variables in theano\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')\n",
    "    xdim = Xtrain.shape[1] # number of features in the data\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    ################### add your code here ###################\n",
    "    ##########################################################\n",
    "    \n",
    "    # HINT : FOR WEIGHTS USE RANDOM STANDRD GAUSSIAN INITIALIZATION\n",
    "    #      : FOR BIAS USE ZERO INITIALIZATION\n",
    "    \n",
    "    # paramters declaration & initialization\n",
    "    # weights from input-neurons in the hidden layer \n",
    "    np.random.seed(1232)\n",
    "    w_1  = #-------------#\n",
    "\n",
    "    # bias to neurons in the hidden layer\n",
    "    b_1  = #-------------#\n",
    "\n",
    "    # weights for hidden - output layer connection\n",
    "    np.random.seed(1232)\n",
    "    w_out = #-------------#\n",
    "\n",
    "    # bias to output layer declaration & initialization\n",
    "    b_out = #-------------#\n",
    "\n",
    "    # hidden layer output activations\n",
    "    h_out = #-------------#\n",
    "\n",
    "    # perceptron predictions\n",
    "    y_pred = #-------------#\n",
    "\n",
    "    # cross-entropy as cost function\n",
    "    cost   = #-------------#\n",
    "\n",
    "    # gradient computation\n",
    "    gw_1, gb_1, gw_out, gb_out = #-------------#\n",
    "    \n",
    "    \n",
    "    # train_model theano function\n",
    "    # Note : outputs should return following in order\n",
    "    #      : [prediction vector, error/cost scalar, hidden layer activation vector]\n",
    "    train_model = theano.function(\n",
    "             inputs  = [x, y],\n",
    "             outputs = [y_pred, cost, h_out],\n",
    "             updates = #-------------#\n",
    "            )\n",
    "\n",
    "    # function \n",
    "    # compute prediction on unseen test data\n",
    "    # Input   : x, y are intput, target vectors respectively\n",
    "    # Output  :  list of predictions\n",
    "    predict_model = theano.function(inputs=[x], outputs=[y_pred])\n",
    "    \n",
    "    # function \n",
    "    # compute cost on test data\n",
    "    # Input   : x, y are intput, target vectors respectively\n",
    "    # Output  : scalar cost\n",
    "    cost_function = theano.function(inputs=[x,y], outputs=cost)\n",
    "    \n",
    "   \n",
    "    ##########################################################\n",
    "    ###################        end         ###################\n",
    "    ##########################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    # accumulate error over iterations on traning and test set in a vector\n",
    "    cost_train_vec = np.array([])\n",
    "    cost_test_vec = np.array([])\n",
    "\n",
    "    for i in np.arange(training_steps):\n",
    "        # get predictions, cost, activation values \n",
    "        # on the training set\n",
    "        # pred_train - vector - predictions on training data\n",
    "        # cost_train  - scalar - cost/error for the current parameter value\n",
    "        # nactivation- vector - activation function from the hidden layer\n",
    "        pred_train, cost_train, nactivation = train_model(\n",
    "            Xtrain, Ytrain)\n",
    "        cost_train_vec = np.append(cost_train_vec, cost_train)\n",
    "          \n",
    "        # get predictions, cost on test set\n",
    "        pred_test = predict_model(Xtest)\n",
    "        cost_test = cost_function(Xtest,Ytest)\n",
    "        cost_test_vec = np.append(cost_test_vec, cost_test)\n",
    "        \n",
    "        # printing\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Iteration %6s -- \"%i,'Training cost: ',\"%4.4f\"%cost_train)\n",
    "    \n",
    "    print(\"final train set cost : %.4f\"%cost_train)\n",
    "    print(\"final test set cost  : %.4f\"%cost_test)\n",
    "    \n",
    "    # compute classification accuracies\n",
    "    train_predictions = (np.round(predict_model(Xtrain)).reshape((1,-1)))\n",
    "    train_accuracy = np.mean(train_predictions == Ytrain)\n",
    "    print(\"final train set classification accuracy : %.4f\"%train_accuracy)\n",
    "    \n",
    "    test_predictions = np.round(pred_test).reshape((1,-1))\n",
    "    test_accuracy = np.mean(test_predictions == Ytest)\n",
    "    print(\"final test set classification accuracy : %.4f\"%test_accuracy)\n",
    "    \n",
    "    # for the final model, plot model fit and activations\n",
    "    # on a grid\n",
    "    X1grid, X2grid = np.meshgrid(\n",
    "        np.linspace(-2, 3, 100), np.linspace(-1.7, 2, 100))\n",
    "    pred_grid = predict_model(\n",
    "        np.transpose(np.array([X1grid.flatten(), X2grid.flatten()])))\n",
    "    pred_grid = np.array(pred_grid)\n",
    "    pred_grid = pred_grid.reshape(X1grid.shape)\n",
    "\n",
    "    plotmodelfit(Xtrain, Ytrain, pred_train,\n",
    "                 [nactivation],\n",
    "                 X1grid, X2grid,pred_grid,\n",
    "                 cost_train_vec, cost_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 02. Run experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single hidden layer ~ 3 neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationSingleHiddenLayerNN(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, nn1=3, training_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single hidden layer ~ 5 neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationSingleHiddenLayerNN(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, nn1=5, training_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single hidden layer ~ 8 neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationSingleHiddenLayerNN(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, nn1=8, training_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Task 03. Implementation with two hidden layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassificationTwoHiddenLayerNN(Xtrain,\n",
    "                                  Ytrain,\n",
    "                                  Xtest,\n",
    "                                  Ytest,\n",
    "                                  nn1=4,\n",
    "                                  nn2=3,\n",
    "                                  training_steps=50000,\n",
    "                                  alpha=0.2):\n",
    "    '''\n",
    "    Input:\n",
    "    Xtrain   : N x D    : traning set features\n",
    "    Ytrian   : N x 1    : training set target\n",
    "    Xtest    : M x D    : test set feaures\n",
    "    Ytest    : M x 1    : test set target\n",
    "    nn1      : scalar   : no. of neurons to be used in first hidden layer\n",
    "    nn2      : scalar   : no. of neurons to be used in second hidden layer\n",
    "    training_steps : scalar : no. of training iteration steps\n",
    "    alpha    : scalar   : learning rate\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    print(\"*** running ***\")\n",
    " \n",
    "    # define input and output variables in theano\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')\n",
    "    xdim = Xtrain.shape[1]  # number of features in the data\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    ################### add your code here ###################\n",
    "    ##########################################################\n",
    "            \n",
    "    # HINT : FOR WEIGHTS USE RANDOM STANDRD GAUSSIAN INITIALIZATION\n",
    "    #      : FOR BIAS USE ZERO INITIALIZATION\n",
    "    \n",
    "    # layer 01 parameter declaration & initialization (weights/bias)\n",
    "    np.random.seed(1232)\n",
    "    w_1 = #-------------#\n",
    "    b_1 = #-------------#\n",
    "\n",
    "    # layer 02 parameter declaration & initialization (weights/bias) \n",
    "    np.random.seed(1232)\n",
    "    w_2 = #-------------#\n",
    "    b_2 = #-------------#\n",
    "\n",
    "    # output layer parameter declaration & initialization (weights/bias) \n",
    "    np.random.seed(1232)\n",
    "    w_out = #-------------#\n",
    "    b_out = #-------------#\n",
    "\n",
    "    # hidden layer output\n",
    "    h_out_1 = #-------------#\n",
    "    h_out_2 = #-------------#\n",
    "\n",
    "    # perceptron predictions\n",
    "    y_pred = #-------------#\n",
    "\n",
    "    # cross-entropy as cost function\n",
    "    cost   = #-------------#\n",
    "\n",
    "    # gradient computation\n",
    "    gw_1, gb_1, gw_2, gb_2, gw_out, gb_out = #-------------#\n",
    "        \n",
    "    # train_model theano function\n",
    "    # Note : outputs should return following in order\n",
    "    #      : [prediction vector, error/cost scalar,\n",
    "    #        1st hidden layer activation vector, 2nd hidden layer activation vector]\n",
    "    train_model = theano.function(\n",
    "             inputs  = [x,y],\n",
    "             outputs = [y_pred, cost, h_out_1, h_out_2],\n",
    "             updates = #-------------#\n",
    "            )\n",
    "    \n",
    "\n",
    "    # function \n",
    "    # compute prediction on unseen test data\n",
    "    # Input   : x, y are intput, target vectors respectively\n",
    "    # Output  :  list of predictions\n",
    "    predict_model = theano.function(inputs=[x], outputs=[y_pred])\n",
    "    \n",
    "    # function \n",
    "    # compute cost on test data\n",
    "    # Input   : x, y are intput, target vectors respectively\n",
    "    # Output  : scalar cost\n",
    "    cost_function = theano.function(inputs=[x,y], outputs=cost)\n",
    "    \n",
    "\n",
    "    ##########################################################\n",
    "    ###################        end         ###################\n",
    "    ##########################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "    # accumulate error over iterations on traning and test set in a vector\n",
    "    cost_train_vec = np.array([])\n",
    "    cost_test_vec = np.array([])\n",
    "\n",
    "    # training iterations begin\n",
    "    for i in np.arange(training_steps):\n",
    "        \n",
    "        # get predictions, cost, activation values \n",
    "        # on the training set\n",
    "        # pred_train - vector - predictions on training data\n",
    "        # cost_train  - scalar - cost/error for the current parameter value\n",
    "        # nactivation- vector - activation function from the hidden layer\n",
    "        pred_train, cost_train, nactivation1, nactivation2 = train_model(\n",
    "            Xtrain, Ytrain)\n",
    "        cost_train_vec = np.append(cost_train_vec, cost_train)\n",
    "            \n",
    "        # get predictions, cost on test set\n",
    "        pred_test = predict_model(Xtest)\n",
    "        cost_test = cost_function(Xtest,Ytest)\n",
    "        cost_test_vec = np.append(cost_test_vec, cost_test)\n",
    "        \n",
    "        # printing\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Iteration %6s -- \"%i,'Training cost: ',\"%4.4f\"%cost_train)\n",
    "\n",
    "    print(\"final train set cost : %.4f\"%cost_train)\n",
    "    print(\"final test set cost  : %.4f\"%cost_test)\n",
    "    \n",
    "    # compute classification accuracies\n",
    "    train_predictions = (np.round(predict_model(Xtrain)).reshape((1,-1)))\n",
    "    train_accuracy = np.mean(train_predictions == Ytrain)\n",
    "    print(\"final train set classification accuracy : %.4f\"%train_accuracy)\n",
    "    \n",
    "    test_predictions = np.round(pred_test).reshape((1,-1))\n",
    "    test_accuracy = np.mean(test_predictions == Ytest)\n",
    "    print(\"final test set classification accuracy : %.4f\"%test_accuracy)\n",
    "    \n",
    "    # for the final model, plot model fit and activations\n",
    "    # on a grid\n",
    "    X1grid, X2grid = np.meshgrid(\n",
    "        np.linspace(-2, 3, 100), np.linspace(-1.7, 2, 100))\n",
    "    pred_grid = predict_model(\n",
    "        np.transpose(np.array([X1grid.flatten(), X2grid.flatten()])))\n",
    "    pred_grid = np.array(pred_grid)\n",
    "    pred_grid = pred_grid.reshape(X1grid.shape)\n",
    "\n",
    "    plotmodelfit(Xtrain, Ytrain, pred_train,\n",
    "                 [nactivation1, nactivation2],\n",
    "                 X1grid, X2grid, pred_grid,\n",
    "                 cost_train_vec, cost_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 04. Run experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two hidden layers ~ 3 and 2 neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationTwoHiddenLayerNN(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, nn1=3, nn2=2, training_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two hidden layers ~ 5 and 3 neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationTwoHiddenLayerNN(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, nn1=5, nn2=3, training_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two hidden layers ~ 10 and 10 neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationTwoHiddenLayerNN(\n",
    "    Xtrain, Ytrain, Xtest, Ytest, nn1=10, nn2=10, training_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 05. Discussion**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1550px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
