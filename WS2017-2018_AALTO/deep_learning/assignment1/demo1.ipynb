{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents :\n",
    "    A brief overview of Theano\n",
    "    \n",
    "    Importing of libraries that we will use in this demo\n",
    "    \n",
    "    1. Steps involved in Theano - Scalar addition and beyond\n",
    "         1.1 Declaring variables\n",
    "         1.2 Symbolically define mathematical expressions (gradients are auto-derived)\n",
    "         1.3 Compile expressions into executable functions\n",
    "         1.4 Execute expression\n",
    "         1.5 A simple matrix addition\n",
    "    \n",
    "    2. Sigmoid function\n",
    "         2.1 Sigmoid function in NumPy\n",
    "         2.2 Sigmoid function in Theano\n",
    "         2.3 Analytical gradient in NumPy\n",
    "         2.4 Symbolic differentiation in Theano\n",
    "    \n",
    "    3. Demo - Regression with a simple perceptron\n",
    "         3.1 Training data\n",
    "         3.2 Architecture\n",
    "         3.3 Formulas and implementation\n",
    "         3.4 Training and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief overview of Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define, optimize, and evaluate mathematical expressions on multi-dimensional arrays__\n",
    "\n",
    "\n",
    "* Integration with NumPy - can use numpy.ndarray in Theano-compiled functions\n",
    "* Uses GPU to scale performance\n",
    "* Efficient symbolic differentiation (No more d/dx!!)\n",
    "\n",
    "__It's a mathematical toolkit__\n",
    "\n",
    "\n",
    "* No inbuilt machine learning models\n",
    "* Developed by MILA lab at the University of Montreal (headed by Yoshua Bengio)\n",
    "\n",
    "__What is NumPy?__\n",
    "\n",
    "\n",
    "* Scientific computing package in Python on the CPU\n",
    "* Provides a powerful N-dimensional array object (ndarray)\n",
    "* Allows broadcasting\n",
    "* Provides extensive Linear algebra methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing of libraries that we will use in this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theano:\n",
    "# -------\n",
    "import theano # (often imported as th)\n",
    "from theano import tensor as T \n",
    "# NumPy: \n",
    "# ------\n",
    "import numpy as np \n",
    "# Matplotlib:\n",
    "# ------------\n",
    "import matplotlib as mt \n",
    "import matplotlib.pyplot as plt # plotting module\n",
    "import matplotlib.image as mpimg # display existing image\n",
    "import matplotlib.gridspec as gridspec # get matplotlib layouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Steps involved in Theano - Scalar addition and beyond\n",
    "To get us started with Theano and get a feel of what we're working with, we will create two\n",
    "simple Theano-functions (adding two scalars, and adding two matrices) and test them out. We will start with the scalar addition, discussing main concepts related while doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Declaring variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano, all symbols must be typed. In particular, `T.dscalar`\n",
    "is the type we assign to \"0-dimensional arrays (`scalar`) of doubles\n",
    "(`d`)\"; it is a Theano `type`. In the few cells below, we declare the scalar variables that we want to later add up symbolically, and demonstrate a bit how Python interprets them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaration of the variables\n",
    "x = T.dscalar('x')\n",
    "y = T.dscalar('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.dscalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Symbolic expressions\n",
    "\n",
    "The next step is to combine *x* and *y* into their sum *z*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x + y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*z* is yet another *Variable* which represents the addition of\n",
    "*x* and *y*. You can use the `pp` function to *pretty-print* out the computation associated to *z*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theano.printing.pp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Compiling a function\n",
    "\n",
    "The last step is to create a Theano-function taking *x* and *y* as inputs\n",
    "and giving *z* as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = theano.function([x, y], z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument to `function()` is a list of Variables\n",
    "that will be provided as inputs to the function. The second argument\n",
    "is a single Variable *or* a list of Variables. For either case, the second\n",
    "argument is what we want to see as output when we apply the function. *f* may\n",
    "then be used like a normal Python function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Execute expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(16.3, 12.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are following along and typing into an interpreter, you may have\n",
    "noticed that there was a slight delay in executing the ``function``\n",
    "instruction. Behind the scenes, *f* was being compiled into C code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, Theano builds a graph structure composed of interconnected `Variable` nodes, `op` nodes and `apply` nodes. \n",
    "\n",
    "An `apply` node represents the application of an `op` to some variables. It is important to draw the difference between the definition of a computation represented by an `op` and its application to some actual data which is represented by the apply node. \n",
    "\n",
    "Here is the expression graph corresponding to the addition of `x` and `y`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theano.printing.pydotprint(f,outfile=\"samplepydot.png\")\n",
    "\n",
    "mt.rcParams['figure.figsize'] = (12,6)\n",
    "img01 = mpimg.imread('samplepydot.png')\n",
    "p01   = plt.imshow(img01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *Variable* is the main data structure you work with when\n",
    "using Theano. By calling `T.dscalar` with a string argument, you create a\n",
    "`Variable` representing a floating-point scalar quantity with the\n",
    "given name. If you provide no argument, the symbol will be unnamed. Names\n",
    "are not required, but they can help debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Adding two matrices\n",
    "\n",
    "Here we consider creating and using a Theano-function for adding two matrices of doubles. We will first define the Theano-function, note that we now need to use the Type ``dmatrix`` for the symbolic variable summands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = T.dmatrix('X')\n",
    "Y = T.dmatrix('Y')\n",
    "Z = X + Y\n",
    "f = theano.function([X, Y], Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Theano-function defined, let's test it on adding up two 2D-arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f([[1, 2], [3, 4]],[[10, 20], [30, 40]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The logistic sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 The function in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now generate a vector of floating point values over which we would evaluate the logistic sigmoid function defined above and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 points equally spaced in [-5,5]\n",
    "x = np.linspace(-5,5,100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sigmoid(x):\n",
    "    return 1./(1. + np.exp(-x)) # note: element-wise evaluation for non-scalars\n",
    "\n",
    "# evaluate the logistic sigmoid function at each of the points\n",
    "np_sigmoid_output = np_sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the evalutions \n",
    "mt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.plot(x,np_sigmoid_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 The function in Theano\n",
    "Let us now define the sigmoid function in **Theano**, note how similar it looks to the NumPy sigmoid function definition.\n",
    "\n",
    "Note that there is an inbuilt function **theano.tensor.nnet.sigmoid** which we will be using in later part of the demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the th_ prefix for theano symbolic variables and functions\n",
    "th_x = T.dscalar('x') \n",
    "th_sigmoid = 1./(1. + T.exp(-th_x))\n",
    "\n",
    "# using the th_ prefix to distinguish theano function names.\n",
    "th_sigmoid_f = theano.function(inputs=[th_x], outputs=th_sigmoid) \n",
    "\n",
    "# We evaluate the theano function by looping over x.\n",
    "th_sigmoid_output = []\n",
    "for element in x:\n",
    "    th_sigmoid_output.append(th_sigmoid_f(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the x evaluated using the sigmoid \n",
    "# function defined in theano\n",
    "plt.plot(x,th_sigmoid_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _No surprise ! The plots look the same as before._\n",
    "Let's quickly check their absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(abs(np_sigmoid_output - th_sigmoid_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Analytical derivative in NumPy\n",
    "We first (manually) derive the analytical form of the derivative of the logistic sigmoid function (denoted here $\\mathrm{sigmoid}(\\cdot)$). Using the knowledge \n",
    "that $\\mathrm{sigmoid}(x) = (1+\\exp(-x))^{-1}$ and that $\\mathrm{sigmoid}(-x) = 1-\\text{sigmoid(x)}$, we can write that \n",
    "$$\n",
    "    \\begin{split}\n",
    "       \\frac{\\partial \\mathrm{sigmoid}(x)}{\\partial x} &= \\frac{\\partial (1+\\exp(-x))^{-1}}{\\partial x}  \\\\\n",
    "        &= (-1)(1+\\exp(-x))^{-2} \\times \\frac{\\partial (1+\\exp(-x))}{\\partial x}  \\\\\n",
    "        &= (-1)(1+\\exp(-x))^{-2} \\times \\exp(-x) \\times \\frac{\\partial (-x))}{\\partial x}  \\\\\n",
    "        &= \\frac{1}{1+\\exp(-x)} \\times \\frac{\\exp(-x)}{1+\\exp(-x)} \\\\\n",
    "        &= \\frac{1}{1+\\exp(-x)} \\times \\left( 1 - \\frac{1}{1+\\exp(-x)} \\right) \\\\ \\\\\n",
    "        &= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x)).\n",
    "    \\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this result to implement a function for evaluating the derivative using **NumPy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sigmoid_grad(x):\n",
    "    return np_sigmoid(x)*(1.-np_sigmoid(x)) # note: element-wise evaluation for non-scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate the function at each of the points defined by the NumPy-variable x, and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, np_sigmoid_grad(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement the analytically calculated gradient using Theano and the output would be same. Try it out for yourself !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Symbolic differentiation in Theano\n",
    "We will now use Theano's automatic differentiation `T.grad(cost,wrt)` to calculate the gradient of the sigmoid function \n",
    "\n",
    "**NOTE:** We will re-use the **sym_sigmoid** Theano expression for sigmoid defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# T.grad computes derivative of 'th_sigmoid' function at values 'th_x'\n",
    "th_sigmoid_grad = T.grad(th_sigmoid,th_x)\n",
    "th_sigmoid_grad_f = theano.function(inputs=[th_x],\n",
    "                                    outputs=th_sigmoid_grad)\n",
    "\n",
    "th_sigmoig_grad_output = [th_sigmoid_grad_f(element) for element in x]\n",
    "plt.plot(x, th_sigmoig_grad_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks same as the analytically calculated gradient !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Demo - Regression with a simple perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load some simulated data for our exercise, here we use NumPy to create a noisy sine wave signal. \n",
    "\n",
    "We will be using the same structure for the data to be provided in the home assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "Xtrain = np.loadtxt(\"files/sinewave_Xtrain.txt\") \n",
    "Ytrain = np.loadtxt(\"files/sinewave_Ytrain.txt\") \n",
    "Xtest = np.loadtxt(\"files/sinewave_Xtest.txt\") \n",
    "Ytest = np.loadtxt(\"files/sinewave_Ytest.txt\") \n",
    "\n",
    "\n",
    "# reshape the dimensions\n",
    "Xtrain = Xtrain.reshape(-1,1)\n",
    "Xtest = Xtest.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "mt.rcParams['figure.figsize'] = (8,5)\n",
    "% matplotlib inline\n",
    "plt.scatter(Xtrain,Ytrain,c = \"#639743\",alpha = 0.9,s=15,label=\"train\")\n",
    "plt.scatter(Xtest,Ytest,c = \"#ffad60\",alpha = 0.9,s=15,label=\"test\")\n",
    "plt.title(\"sine wave data demo regression\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block provides functionality to visualize training of single layer perceptrons with 1-D input. Understanding of the plotting code is not needed for the exercise. However don't forget to run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodel1D(Xtrain, Ytrain, Xtest, Ytest, pred_train, pred_test, err_train,\n",
    "                err_test, nactivation, training_steps):\n",
    "    mt.rcParams['figure.figsize'] = (6, 8)\n",
    "    '''\n",
    "    Xtrain : N x 1 array : train input vector\n",
    "    Ytrain : N x 1 array : train target vector\n",
    "    Xtest  : N x 1 array : test input vector\n",
    "    Ytest  : N x 1 array : test target vector\n",
    "    pred_train : N x 1 array : predictions on train set\n",
    "    pred_test  : N x 1 array : predictions on test set\n",
    "    err_train  : scalar : error on train set for the current iteration\n",
    "    err_test   : scalar : error on test set for the current iteration\n",
    "    nactivation  : N x h array : activation output from 'h' hidden neurons on the training data\n",
    "    training_steps : scalar : maximum number of iterations, to set the range of x axis in the beginning\n",
    "    '''\n",
    "\n",
    "    if plt.fignum_exists(121):\n",
    "        fig = plt.figure(121)\n",
    "\n",
    "        ax1 = fig.axes[0]\n",
    "        ax1.lines[0].set_ydata(pred_train)\n",
    "\n",
    "        ax2 = fig.axes[1]\n",
    "        for line, nact in zip(ax2.lines, nactivation.T):\n",
    "            line.set_ydata(nact)\n",
    "\n",
    "        ax3 = fig.axes[2]\n",
    "        ax3.lines[0].set_xdata(\n",
    "            np.append(ax3.lines[0].get_xdata(),\n",
    "                      ax3.lines[0].get_xdata()[-1] + 1000))\n",
    "        ax3.lines[0].set_ydata(np.append(ax3.lines[0].get_ydata(),\n",
    "                                         err_train))\n",
    "        \n",
    "        ax3.lines[1].set_xdata(\n",
    "            np.append(ax3.lines[1].get_xdata(),\n",
    "                      ax3.lines[1].get_xdata()[-1] + 1000))\n",
    "        ax3.lines[1].set_ydata(np.append(ax3.lines[1].get_ydata(),\n",
    "                                         err_test))\n",
    "\n",
    "    else:\n",
    "        itr = 1\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, num=121)\n",
    "\n",
    "        ax1.set_title(\"Demo : A simple perceptron\")\n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.set_xlim(1.3 * Xtrain.min(), 1.05 * Xtrain.max())\n",
    "        ax1.set_ylim(1.3 * Ytrain.min(), 1.3 * Ytrain.max())\n",
    "        ax1.scatter(\n",
    "            Xtrain, Ytrain, c=\"#639743\", alpha=0.5, s=15, label=\"train\")\n",
    "        ax1.scatter(Xtest, Ytest, c=\"#ffad60\", alpha=0.5, s=15, label=\"test\")\n",
    "        ax1.plot(\n",
    "            Xtrain,\n",
    "            pred_train,\n",
    "            \"--\",\n",
    "            c=\"#970000\",\n",
    "            linewidth=2.0,\n",
    "            label=\"model fit\")\n",
    "        ax1.legend(loc=4)\n",
    "\n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('hidden layer activations')\n",
    "        ax2.set_ylim(-0.1, 1.1)\n",
    "        ax2.plot(Xtrain, nactivation)\n",
    "\n",
    "        ax3.set_xlabel('number of iterations')\n",
    "        ax3.set_ylabel('mse')\n",
    "        ax3.plot(itr, err_train, c=\"#27ae61\", label=\"train\")\n",
    "        ax3.plot(itr, err_test, c=\"#c1392b\", label=\"test\")\n",
    "        ax3.set_ylim(-1, max(err_train, err_test) * 1.2)\n",
    "        ax3.set_xlim(1, training_steps * 1000)\n",
    "        ax3.legend()\n",
    "\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below section, a single layer perceptron with architecture as shown below is implemented. One hidden layer with sigmoidal activation function transforms data non-linearly. Outputs from each of these neuron is scaled with some weights along with a bias and summed to form the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='files/arch1.jpg',height=150,width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Formulas and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **bold**, **bold_i** represent vector valued variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data :**\n",
    "\n",
    "   **X**     : Input data (N x 1)\n",
    "   \n",
    "   **y**     : Target vector (N x 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Parameters to be learned : **\n",
    "\n",
    "   $\\text{w}_i$     : Weight for $i^{th}$ input-hidden layer connection\n",
    "   \n",
    "   $\\text{b}_i$     : Bias for $i^{th}$ hidden layer activations\n",
    "   \n",
    "   $\\text{wout}_i$  : Weight for $i^{th}$ hidden-output layer connection\n",
    "   \n",
    "   $\\text{bout}$  : Bias for output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass computations:**\n",
    "\n",
    "   $\\textbf{hout}_i$ : Activation from $i^{th}$ neuron in the hidden layer (N x 1)\n",
    "$$\n",
    "    \\begin{split}\n",
    "        \\textbf{hout}_i &= \\text{sigmoid  }(\\textbf{X} \\cdot \\text{w}_i+ \\text{b}_i)\n",
    "    \\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **ypred** : Output layer as weighted sum of hidden layer activations, along with output layer bias (N x 1)\n",
    "$$\n",
    "    \\begin{split}\n",
    "        \\textbf{ypred} &= \\sum_i \\textbf{hout}_i \\cdot \\text{wout}_i + \\text{bout}\n",
    "    \\end{split}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost function :**\n",
    "\n",
    " Mean squared error is considered as the cost function in this problem\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{cost} &= \\frac{1}{N} \\sum_{j = 1} ^N (\\text{y}_j - \\text{ypred}_j)^2\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Parameter updates for backpropagation:**\n",
    "\n",
    "  We will be using gradient descent backpropagation algorithm. In gradient descent, after every iteration, we look at negative slope (the direction in which the cost function reduces) and take small steps in that direction. The step size is defined by a parameter $\\alpha$ (also called as learning rate). \n",
    "  \n",
    "  Every parameter to be learned is updated after t^th iteration according to the following formula\n",
    "    \n",
    "$$\n",
    "\\begin{split}\n",
    "   \\text{param}_{t+1} &= \\text{param}_t - \\alpha \\cdot \\text{gradient} \\\\\n",
    "   \\text{gradient} &= \\frac{\\partial \\text{ (cost) }}{\\partial \\text{ param }}\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets go ahead and implement this in Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output variables in Theano\n",
    "# value: input data\n",
    "# shape : N x 1  (same as NumPy array)\n",
    "X = T.matrix('X') \n",
    "\n",
    "# value : target vector\n",
    "# shape : N x __ (same as NumPy array)\n",
    "y = T.vector('y') # will have N x __ shape\n",
    "\n",
    "\n",
    "# parameters to be set for the architecture/training\n",
    "nn1 = 4 # NumberOfHiddenNeurons\n",
    "training_steps = 50000  # number of training iterations\n",
    "alpha = 0.1  # learning rate\n",
    "\n",
    "\n",
    "# NOTE : \n",
    "# variables of theano type 'dmatrix' are not broadcastable\n",
    "# initializing a theano shared variable with a NumPy array of shape (N,1) makes it a 'dmatrix'\n",
    "# whenever broadcasting is needed, initialize it as a vector\n",
    "# initializing a theano shared variable with a NumPy array of shape (N,) makes it a 'dvector'\n",
    "\n",
    "# weights for input - the hidden layer connection\n",
    "# value : random gaussian initialization\n",
    "# shape : (1 x NumberOfHiddenNeurons)\n",
    "# type  : dmatrix\n",
    "np.random.seed(1232)\n",
    "w1 = theano.shared(np.random.randn(1, nn1), name='w1') \n",
    "\n",
    "\n",
    "# bias to neurons in the hidden layer\n",
    "# value : initialized to zeros\n",
    "# shape : (NumberOfHiddenNeurons x __)\n",
    "# type  : dvector\n",
    "b1 = theano.shared(np.zeros((nn1,)), name='b1')       \n",
    "\n",
    "\n",
    "# weights for hidden - output layer connection\n",
    "# value : random gaussian initialization\n",
    "# shape : (NumberOfHiddenNeurons x __)\n",
    "# type  : dvector\n",
    "np.random.seed(1232)\n",
    "wout = theano.shared(\n",
    "    np.random.randn(nn1,), name='wout')                           \n",
    "\n",
    "\n",
    "# bias to output layer\n",
    "# value : initialized to zero\n",
    "# shape : (1 x __)\n",
    "# type  : dvector\n",
    "bout = theano.shared(0., name='bout') \n",
    "\n",
    "\n",
    "# hidden layer output\n",
    "# value :  X*w1 + b1 \n",
    "# shape : (N x 1)*(1 x NumberOfHiddenNeurons) + (NumberOfHiddenNeurons x __)\n",
    "#       : (N x NumberOfHiddenNeurons) + (NumberOfHiddenNeurons x __) {BROADCAST}\n",
    "#       : (N x NumberOfHiddenNeurons) \n",
    "# type  : dmatrix\n",
    "# http://deeplearning.net/software/theano/tutorial/broadcasting.html\n",
    "hout = theano.tensor.nnet.sigmoid(T.dot(X, w1) + b1)   \n",
    "\n",
    "\n",
    "# perceptron predictions\n",
    "# value : hout * wout + bout\n",
    "# shape : (N x NumberOfHiddenNeurons) * (NumberOfHiddenNeurons x __) + (1 X __)\n",
    "#       : (N x __) + (1 X __) {BROADCAST}\n",
    "#       : (N x __)\n",
    "# type  : dvector\n",
    "# --- Note that ypred is of same shape as y --- \n",
    "ypred = T.dot(hout, wout) + bout\n",
    "test = theano.function(inputs=[X], outputs=[ypred])\n",
    "\n",
    "\n",
    "# mean squared error as cost function\n",
    "cost = T.mean((ypred - y)**2)\n",
    "\n",
    "\n",
    "# gradient computation : gradient of cost function with respect to weights & bias\n",
    "# used for parameter updates according to gradient descent algorithm\n",
    "gw1, gb1, gwout, gbout = T.grad(cost, [w1, b1, wout, bout])\n",
    "\n",
    "\n",
    "# now we connect everything into a single theano function\n",
    "# Input   :  X, y are input data, target vector respectively\n",
    "# Output  :  prediction, cost (error) and hidden-layer activations\n",
    "# Update  :  update equation for parameters to be learned : w_t+1 = w_t - (learning_rate * gradient)\n",
    "updates  =  [(w1, w1 - alpha * gw1), \n",
    "             (b1, b1 - alpha * gb1),\n",
    "             (wout, wout - alpha * gwout), \n",
    "             (bout, bout - alpha * gbout)] \n",
    "\n",
    "train_model = theano.function(\n",
    "    inputs=[X, y],\n",
    "    outputs=[ypred, cost, hout,wout,bout],\n",
    "    updates=updates)\n",
    "\n",
    "\n",
    "# function \n",
    "# compute prediction on unseen test data\n",
    "# Input   : x, y are input, target vectors respectively\n",
    "# Output  :  list of predictions\n",
    "predict_model = theano.function(inputs=[X], outputs=[ypred])\n",
    "\n",
    "\n",
    "# function \n",
    "# compute cost on test data\n",
    "# Input   : x, y are input, target vectors respectively\n",
    "# Output  : scalar cost\n",
    "cost_function = theano.function(inputs=[X,y], outputs=cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Training and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, we have defined everything in Theano as symbolic variables. Next we execute *train_model* function defined in Theano with NumPy arrays Xtrain and Ytrain as inputs. \n",
    "\n",
    "Function is called iteratively, gradients of cost w.r.t weight/bias parameters are computed after each iteration. Parameters are updated according to the '*updates*' argument eventually learning values that minimize the cost function.\n",
    "\n",
    "Then *predict_model* function is used to get model predictions on the test set and errors on both training and test set across iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# call traning function for pre-defined iterations\n",
    "for ii in np.arange(1,training_steps+1):\n",
    "\n",
    "    # run train function and get predictions and error on train set\n",
    "    # nactivation is the hidden layer neuron activations for the second subplot\n",
    "    # nwout, nbout are hidden-output layer weights and bias respectively\n",
    "    pred_train, cost_train, nactivation, nwout, nbout = train_model(Xtrain, Ytrain)\n",
    "\n",
    "    # get predictions on test set\n",
    "    pred_test = predict_model(Xtest)\n",
    "    cost_test = cost_function(Xtest,Ytest)\n",
    "\n",
    "    if (ii==1 or ii % 1000 == 0):\n",
    "        plotmodel1D(Xtrain, Ytrain, Xtest, Ytest, \n",
    "                    pred_train, pred_test, cost_train, cost_test,\n",
    "                    nactivation, training_steps/1000)\n",
    "        print(\"Iteration %6s -- \"%ii,'Training cost: ',\"%4.4f\"%cost_train)\n",
    "\n",
    "print(\"final train set cost : %.4f\"%cost_train)\n",
    "print(\"final test set cost  : %.4f\"%cost_test)\n",
    "print(\"output layer weights : \",nwout)\n",
    "print(\"output layer bias : \",nbout)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
