{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with your name and student number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pagebreak$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of Fully Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will use the the model explained in the demonstration and apply some optimization techniques for empirical risk minimization by:\n",
    "\n",
    "1. Tuning the bias initializations using grid search\n",
    "2. Implementing the model of momentum and Adam to accelerate learning\n",
    "\n",
    "Remember the model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/convnetStructureforAssignment.png',height=800,width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin the assignment by importing necessary python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six.moves.cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import os, sys\n",
    "import numpy as np \n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv2d\n",
    "from exercise_helper import load_data, conv_layer\n",
    "print('***** Import complete *****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-Batch Gradient Descent** was already implemented in both this week and last week's demos. It is also used as an optimization technique. It updates weights incrementally after each iteration and calculates the cost over mini batches. In the below function, *updates* variable shows the update operation for each parameter (weights and biases) based on the calculated cost with the learning rate. \n",
    "You can use this model as a hint for the rest of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_updates_sgd(cost, params, learning_rate):\n",
    "    # Function to return an update list for the parameters to be updated\n",
    "    \n",
    "    # Inputs:\n",
    "    # cost: MSE cost Theano variable\n",
    "    # params :  parameters coming from hidden and output layers\n",
    "    # learning rate: learning rate defined as hyperparameter\n",
    "    \n",
    "    # Outputs:\n",
    "    # updates : updates to be made and to be defined in the train_model function. \n",
    "    updates = []\n",
    "    # just gradient descent on cost\n",
    "    for param in params:\n",
    "        # use append function to make a list for which Theano variables will be\n",
    "        # updated with which value\n",
    "        updates.append((param, param - learning_rate*T.grad(cost, param)))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter Initialization\n",
    "Now we will create a function *run_convnet()* to run the experiments. The inside of the function is a little bit different than the demo. First, there is two different ways to initialize the output layer bias outside the *conv_layer()* operation. **Note that you have to write the non-shared bias initialization by yourself. You can return the demo for more explanation of the shared and non-shared bias initialization types. ** Secondly, the function updates weights and biases based on the *momentum_type* parameter. Check the *gradient_updates_sgd* code above to get a hint for other parameter update functions.\n",
    "In the non-shared initialization mode, the biases will start from the same value; however now we will have 28x28 matrix, so there will be a different bias for every neuron in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_convnet(learning_rate, num_epochs, \n",
    "                train_set_x,\n",
    "                num_filters, batch_size, \n",
    "                momentum_type, bias_type, bias_init=None):\n",
    "    # Function to create the convolutional neural network, train and\n",
    "    # evaluate it. \n",
    "    \n",
    "    # Inputs:\n",
    "    # learning_rate - Learning rate for Stochastic Gradient Descent\n",
    "    # num_epochs - Number of training epochs\n",
    "    # train_set_x - training set\n",
    "    # num_filters - Number of channels for each convolution layer\n",
    "    #               for e.g. 2 layers - [20, 50]. \n",
    "    #.              layer1 = 20, layer2 = 50\n",
    "    # batch_size - Mini-batch size to be used\n",
    "    # momentum_type - Parameter update algorithm to be used\n",
    "    # bias type - bias initialization type to be used \n",
    "    #                shared or non-shared\n",
    "    # bias_init - initial value for bias\n",
    "    \n",
    "    # Outputs:\n",
    "    # Training MSE for each iteration\n",
    "    \n",
    "    # random seed to initialize the pseudo-random number generator.\n",
    "    rng = np.random.RandomState(23455)\n",
    "\n",
    "    # compute number of minibatches for training and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    # get the dimensions for input images\n",
    "    import math\n",
    "    D = train_set_x.get_value(borrow=True).shape[1]\n",
    "    L = train_set_x.get_value(borrow=True).shape[0]\n",
    "    W = int(math.sqrt(D))\n",
    "    assert W * W == D\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    # minibatch index\n",
    "    index = T.lscalar() \n",
    "    x = T.matrix('x')\n",
    "\n",
    "        \n",
    "    # reshape matrix of rasterized images of shape (batch_size x W x W), W=28\n",
    "    # to a 4D tensor to produce MNIST images with a size of  \n",
    "    # (mini_batch_size x 1 x 28 x 28)\n",
    "    input_layer = x.reshape((batch_size, 1, W, W))\n",
    "\n",
    "    # binarize the hidden layer 4D tensor with uniform distribution\n",
    "    input_layer_binarized = ((input_layer + \n",
    "                                    np.random.rand(batch_size,1,W,W)) > \n",
    "                                    1.0).astype(theano.config.floatX)\n",
    "    \n",
    "\n",
    "    # construct the first convolutional layer:\n",
    "    # filtering reduces the image size to (24, 24)\n",
    "    # no pooling\n",
    "    # 4D output tensor is thus of shape (mini_batch_size, num_filters[0], 24, 24)\n",
    "    [hidden_layer_output, \n",
    "    hidden_layer_params] = conv_layer(\n",
    "                                    rng, input=input_layer_binarized, \n",
    "                                    image_shape=(batch_size, 1, 28, 28), \n",
    "                                    filter_shape=(num_filters[0], 1, 5, 5), \n",
    "                                    border_mode='valid', \n",
    "                                    activation = T.tanh, bias=None)\n",
    "    \n",
    "    # check the bias type (shared, or non-shared)\n",
    "    # if it is shared, create bias of shape (num_filters[1])\n",
    "    # if not shared, you need to create bias of shape (image_size)\n",
    "    # this bias initialization type will only applied to the output layer\n",
    "    if (bias_type == 'shared') and (bias_init is not None):\n",
    "        bias_init = np.ones((num_filters[1],),\n",
    "                            dtype=theano.config.floatX)*bias_init\n",
    "    elif (bias_type == 'non-shared') and (bias_init is not None):\n",
    "        \n",
    "        ################# Insert Your Code Here ###################\n",
    "        bias_init =    \n",
    "        ################# Insert Your Code Here ###################\n",
    "\n",
    "    # construct the second convolutional layer for output\n",
    "    # filtering increases the image size to (28, 28)\n",
    "    # no pooling\n",
    "    # 4D output tensor is thus of shape (mini_batch_size, num_filters[1], 28, 28)\n",
    "    [output, \n",
    "    output_layer_params] = conv_layer(\n",
    "                                    rng, \n",
    "                                    input=hidden_layer_output, \n",
    "                                    image_shape=(batch_size, num_filters[0], 24, 24), \n",
    "                                    filter_shape=(num_filters[1], \n",
    "                                    num_filters[0], 5, 5), \n",
    "                                    border_mode='full', \n",
    "                                    activation = T.nnet.sigmoid, bias=bias_init)\n",
    "\n",
    "    # compute the cost (Mean Square Error) to be optimized\n",
    "    cost = T.mean((x.flatten(2) - output.flatten(2)) ** 2)\n",
    "    # create a list of all model parameters to be fit by gradient descent\n",
    "    params = output_layer_params + hidden_layer_params\n",
    "    \n",
    "    # check the parameter update techniques \n",
    "    # and run the related function to update parameters\n",
    "    if momentum_type == 'sgd':\n",
    "            updates = gradient_updates_sgd(cost, params, learning_rate)\n",
    "    elif momentum_type == 'momentum':\n",
    "            updates = gradient_updates_momentum(cost, params, learning_rate)\n",
    "    elif momentum_type == 'Adam':\n",
    "            updates = gradient_updates_Adam(cost, params, learning_rate)\n",
    "            \n",
    "\n",
    "    train_model = theano.function(\n",
    "                    [index],\n",
    "                    cost,\n",
    "                    updates=updates,\n",
    "                    givens={x: train_set_x[index * batch_size: (index + 1) * batch_size]})\n",
    "    \n",
    "    # test_ model function is initialized and called for plotting the reconstructed images\n",
    "    ind = np.random.randint(n_test_batches)\n",
    "    test_model = theano.function([],\n",
    "        [input_layer, \n",
    "        input_layer_binarized, output],\n",
    "        givens={x: test_set_x[ind * batch_size: (ind + 1) * batch_size]})\n",
    "    \n",
    "\n",
    "    print('...training model...')\n",
    "\n",
    "    epoch = 0\n",
    "    train_mse = []\n",
    "    while (epoch < num_epochs):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            train_mse = np.append(train_mse, train_model(minibatch_index))\n",
    "            \n",
    "    [original_input, binarized_input, predicted_output] = test_model()\n",
    "\n",
    "    print('***** Training Complete *****')\n",
    "    return train_mse,[original_input, binarized_input, predicted_output] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load a subset of the full dataset. Please check that hyperparameters, such as number of filters for each layer and the learning rate are different from the one in the demo so that you can easily visualize and interpret the differences between optimization algotihms. **Now define an array for the *bias_init_search_array* parameter.** It could be a type of *numpy.asarray()* or *numpy.linspace()* functions or a simple list. Start from negative values and continue to positive values. Define at least 5 different values (5-10 values) for this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and only get the training data\n",
    "dataset = 'mnist.pkl.gz'\n",
    "datasets = load_data(dataset)\n",
    "\n",
    "train_set_x = datasets[0]\n",
    "test_set_x = datasets[2]\n",
    "print('Training set: %d samples'\n",
    "  %(train_set_x.get_value(borrow=True).shape[0])) \n",
    "print('Test set: %d samples'\n",
    "  %(test_set_x.get_value(borrow=True).shape[0])) \n",
    "\n",
    "# Define hyperparameters\n",
    "num_filters = [6, 1]\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "\n",
    "################# Insert Your Code Here ###################\n",
    "bias_init_search_array = \n",
    "################# Insert Your Code Here ###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the training MSE error for each shared-type bias initialization for the output layer and plot the MSE for each iteration. **Run the code snippet below and comment on the results in the Conclusions section at the end of this assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure for plots\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "# run experiments and add the resulting MSE to the plot function\n",
    "for bias in bias_init_search_array:\n",
    "    print('for shared bias %f' %bias)\n",
    "    train_mse_for_iterations, _ = run_convnet(\n",
    "                                          learning_rate, \n",
    "                                          num_epochs, \n",
    "                                          train_set_x,\n",
    "                                          num_filters, \n",
    "                                          batch_size, \n",
    "                                          momentum_type='sgd', \n",
    "                                          bias_type='shared', bias_init=bias)\n",
    "    \n",
    "    print('Training MSE after training is done: %f' \n",
    "          %train_mse_for_iterations[-1])\n",
    "    plt.plot(train_mse_for_iterations, label = 'bias:'+str(bias))\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.title('Training MSE over Iterations With Different Shared Biases')\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are testing the non-shared initialization for the bias in the output layer. You will use the same values defined previously. **Remember that, in the *run_convnet()* function of Part 1, you have to fill in the initialization line for this mode. Again, after plotting the results, add your comments to the Conclusion section** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure for plots\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "# run experiments and add the resulting MSE to the plot function\n",
    "for bias in bias_init_search_array:\n",
    "    print('for non-shared 28x28 bias %f' %bias)\n",
    "    train_mse_for_iterations, _ = run_convnet(\n",
    "                                            learning_rate, \n",
    "                                            num_epochs, \n",
    "                                            train_set_x,  \n",
    "                                            num_filters, \n",
    "                                            batch_size, momentum_type='sgd', \n",
    "                                            bias_type='non-shared', bias_init=bias)\n",
    "    \n",
    "    print('Training MSE after training is done: %f' %train_mse_for_iterations[-1])\n",
    "    plt.plot(train_mse_for_iterations, label = 'bias(28x28): '+str(bias))\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.title('Training MSE over Iterations With Different NonShared Biases')\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameter Update\n",
    "In the second part of the assignment you will write parameter update algorithms for both momentum and Adam techniques. Please check the *gradient_update_sgd()* function, hints given below for writing your codes, as well as read section 8.3 and 8.6 of the deep learning book for more detailed information on these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_updates_momentum(cost, params, learning_rate, momentum=0.9):\n",
    "    # Function to return an update list for the parameters to be updated\n",
    "    \n",
    "    # Inputs:\n",
    "    # cost: MSE cost Theano variable\n",
    "    # params :  parameters coming from hidden and output layers\n",
    "    # learning rate: learning rate defined as hyperparameter\n",
    "    # momentum : momentum parameter, \n",
    "    #            usually a high value (0.8, 0.9) was chosen for momentum \n",
    "    # Outputs:\n",
    "    # updates : updates to be made and to be defined in the train_model function \n",
    "    updates = []\n",
    "    for param in params:\n",
    "        # for each parameter, we'll create a velocity shared variable\n",
    "        # since we need to remember the velocity and update in each iteration\n",
    "        # it should be the same size with the param\n",
    "        # we initialize it to 0\n",
    "        velocity = theano.shared(param.get_value(borrow=True)*0.)\n",
    "\n",
    "        # hint 1: remember the momentum algorithm \n",
    "        # for each parameter: \n",
    "        # compute gradient estimate \n",
    "        # compute velocity update\n",
    "        # compute parameter update\n",
    "        \n",
    "        # hint2 : use updates.append() function similar to the \n",
    "        # gradient_updates_sgd() function above\n",
    "        \n",
    "        ################# Insert Your Code Here ###################\n",
    "\n",
    "        \n",
    "        ################# Insert Your Code Here ###################\n",
    "\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_updates_Adam(cost, params, learning_rate):\n",
    "    # Function to return an update list for the parameters to be updated\n",
    "    \n",
    "    # cost: MSE cost Theano variable\n",
    "    # params :  parameters coming from hidden and output layers\n",
    "    # learning rate: learning rate defined as hyperparameter\n",
    "    \n",
    "    # Outputs:\n",
    "    # updates : updates to be made and to be defined in the train_model function. \n",
    "    updates = []\n",
    "    eps = 1e-4 # small constant used for numerical stabilization.\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    # beta1 and beta2 are the exponential decay rates \n",
    "    # for moment estimates, in [0,1).\n",
    "    # suggested defaults: 0.9 and 0.999 respectively\n",
    "    for param in params:\n",
    "            # hint 1: create a shared variable for time step\n",
    "            # initialize time step t = 0\n",
    "            \n",
    "            # hint 2: create shared variables for 1st and 2nd moment variables\n",
    "            # they should be the same size with the param\n",
    "            # initialize 1st and 2nd moment variables s = 0, r = 0\n",
    "            \n",
    "            # hint 3: the initializations of these parameters\n",
    "            # will follow the same structure in momentum function \n",
    "            # (check the velocity initialization part)\n",
    "            \n",
    "            # hint 4: remember the Adam algorithm\n",
    "            #         compute gradient\n",
    "            #         update biased first moment estimate\n",
    "            #         update biased second moment estimate\n",
    "            #         correct bias in first moment\n",
    "            #         correct bias in second moment\n",
    "            #         compute parameter update\n",
    "            \n",
    "            ################# Insert Your Code Here ###################\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ################# Insert Your Code Here ###################\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will run experiments for simple gradient descent, momentum and Adam parameter update techniques.\n",
    "Here, you will choose a *bias_init* value based on the results from the previous part. **Find out the best value for output bias initialization (shared version) and use the same value in the runs below.** When you run each cell, you will get a separate plot for for each method. **After getting this plot, comment on the results in Conclusion section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Insert Your Code Here ###################\n",
    "bias_init = \n",
    "################# Insert Your Code Here ###################\n",
    "sgd_train_mse_for_iterations, _ = run_convnet(\n",
    "                                            learning_rate, \n",
    "                                            num_epochs, \n",
    "                                            train_set_x, \n",
    "                                            num_filters, \n",
    "                                            batch_size, momentum_type='sgd', \n",
    "                                            bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with sgd is done: %f' \n",
    "      %sgd_train_mse_for_iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_train_mse_for_iterations, _ = run_convnet(\n",
    "                                                learning_rate, \n",
    "                                                num_epochs, \n",
    "                                                train_set_x, \n",
    "                                                num_filters, \n",
    "                                                batch_size, momentum_type='momentum', \n",
    "                                                bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with momentum is done: %f' \n",
    "      %momentum_train_mse_for_iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_train_mse_for_iterations, _ = run_convnet(\n",
    "                                            learning_rate, \n",
    "                                            num_epochs, \n",
    "                                            train_set_x, \n",
    "                                            num_filters, \n",
    "                                            batch_size, momentum_type='Adam', \n",
    "                                            bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with Adam is done: %f' \n",
    "      %adam_train_mse_for_iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below is for plotting the loss values during the training with different parameter update techniques. There will be no change for this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(sgd_train_mse_for_iterations, label = 'SGD')\n",
    "plt.plot(momentum_train_mse_for_iterations, label = 'Momentum')\n",
    "plt.plot(adam_train_mse_for_iterations, label = 'Adam')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.title('Training MSE over Iterations With Different Parameter Update Techniques (learning rate : 0.01)')\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will repeat the experiments with different learning rates = 0.1 and 0.001. After getting the results from parameter update techniques used with different learning rates, report the best model out of these nine models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "print('for learning rate %f:' %(learning_rate))\n",
    "sgd_train_mse_for_iterations, _ = run_convnet(\n",
    "                                            learning_rate, \n",
    "                                            num_epochs, \n",
    "                                            train_set_x, \n",
    "                                            num_filters, \n",
    "                                            batch_size, momentum_type='sgd', \n",
    "                                            bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with sgd is done: %f' \n",
    "      %sgd_train_mse_for_iterations[-1])\n",
    "\n",
    "momentum_train_mse_for_iterations, _ = run_convnet(\n",
    "                                                learning_rate, \n",
    "                                                num_epochs, \n",
    "                                                train_set_x, \n",
    "                                                num_filters, \n",
    "                                                batch_size, momentum_type='momentum', \n",
    "                                                bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with momentum is done: %f' \n",
    "      %momentum_train_mse_for_iterations[-1])\n",
    "\n",
    "adam_train_mse_for_iterations, _ = run_convnet(\n",
    "                                            learning_rate, \n",
    "                                            num_epochs, \n",
    "                                            train_set_x, \n",
    "                                            num_filters, \n",
    "                                            batch_size, momentum_type='Adam', \n",
    "                                            bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with Adam is done: %f' \n",
    "      %adam_train_mse_for_iterations[-1])\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(sgd_train_mse_for_iterations, label = 'SGD')\n",
    "plt.plot(momentum_train_mse_for_iterations, label = 'Momentum')\n",
    "plt.plot(adam_train_mse_for_iterations, label = 'Adam')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.title('Training MSE over Iterations With Different Parameter Update Techniques (learning rate : 0.1)')\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "print('for learning rate %f:' %(learning_rate))\n",
    "sgd_train_mse_for_iterations, _ = run_convnet(\n",
    "                                            learning_rate, \n",
    "                                            num_epochs, \n",
    "                                            train_set_x, \n",
    "                                            num_filters, \n",
    "                                            batch_size, momentum_type='sgd', \n",
    "                                            bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with sgd is done: %f' \n",
    "      %sgd_train_mse_for_iterations[-1])\n",
    "\n",
    "momentum_train_mse_for_iterations, _ = run_convnet(\n",
    "                                                learning_rate, \n",
    "                                                num_epochs, \n",
    "                                                train_set_x, \n",
    "                                                num_filters, \n",
    "                                                batch_size, momentum_type='momentum', \n",
    "                                                bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with momentum is done: %f' \n",
    "      %momentum_train_mse_for_iterations[-1])\n",
    "\n",
    "adam_train_mse_for_iterations, _ = run_convnet(\n",
    "                                            learning_rate, \n",
    "                                            num_epochs, \n",
    "                                            train_set_x, \n",
    "                                            num_filters, \n",
    "                                            batch_size, momentum_type='Adam', \n",
    "                                            bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "print('Training MSE after training with Adam is done: %f' \n",
    "      %adam_train_mse_for_iterations[-1])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(sgd_train_mse_for_iterations, label = 'SGD')\n",
    "plt.plot(momentum_train_mse_for_iterations, label = 'Momentum')\n",
    "plt.plot(adam_train_mse_for_iterations, label = 'Adam')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.title('Training MSE over Iterations With Different Parameter Update Techniques (learning rate : 0.001)')\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, select the best model giving the minimum MSE by looking the parameter update technique and the learning rate, then run the code snippet below by inserting the parameters for your best model in order to see the predicted images from their binarized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Insert Your Code Here ###################\n",
    "learning_rate = \n",
    "momentum_type_string = \n",
    "################# Insert Your Code Here ###################\n",
    "_, [original_input, binarized_input, predicted_output] = run_convnet(\n",
    "                                    learning_rate, \n",
    "                                    num_epochs, \n",
    "                                    train_set_x, \n",
    "                                    num_filters, \n",
    "                                    batch_size, momentum_type=momentum_type_string, \n",
    "                                    bias_type='shared', bias_init=bias_init)\n",
    "\n",
    "# four axes, returned as a 2-d array\n",
    "f, axarr = plt.subplots(3, 5, figsize=(8, 8))\n",
    "for i in range(5):\n",
    "    axarr[0,i].imshow(\n",
    "        original_input[i].reshape(28,28).astype('float32'), cmap='gray')\n",
    "    axarr[1,i].imshow(\n",
    "        binarized_input[i].reshape(28,28).astype('float32'), cmap='gray')\n",
    "    axarr[2,i].imshow(\n",
    "        predicted_output[i].reshape(28,28).astype('float32'), cmap='gray')\n",
    "    # Turn off tick labels\n",
    "    axarr[0,i].set_yticklabels([])\n",
    "    axarr[0,i].set_xticklabels([])\n",
    "    axarr[1,i].set_yticklabels([])\n",
    "    axarr[1,i].set_xticklabels([])\n",
    "    axarr[2,i].set_yticklabels([])\n",
    "    axarr[2,i].set_xticklabels([])\n",
    "axarr[0,2].set_title('Original Image', fontsize=15)\n",
    "axarr[1,2].set_title('Binarized Image', fontsize=15)\n",
    "axarr[2,2].set_title('Reconstructed Image', fontsize=15)\n",
    "plt.suptitle('Reconstructed MNIST Images', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this text with the details and discussions on the experiments above and write a final conclusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
